import certifi
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.genai as genai
from google.api_core.exceptions import ResourceExhausted
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import traceback

from flask import Flask
import threading

app = Flask(__name__)

@app.route("/")
def home():
    return "Bot is running"

# ‚Äî‚Äî‚Äî ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® (Render Environment Variables ‡§¨‡§æ‡§ü ‡§™‡§¢‡•ç‡§®‡•á) ‚Äî‚Äî‚Äî
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # Simplified to a single key
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# ‚Äî‚Äî‚Äî INITIALIZATION (Render-optimized) ‚Äî‚Äî‚Äî
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# Configure Gemini with the single API key
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("WARNING: GEMINI_API_KEY environment variable not set.")

MODELS = ["gemini-1.5-flash", "gemini-1.5-pro"]

# ‚Äî‚Äî‚Äî CORE HELPER FUNCTIONS ‚Äî‚Äî‚Äî

def log_exception(e):
    """‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≤‡§ó ‡§ó‡§∞‡•ç‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø‡•§"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def get_embedding(text):
    """‡§™‡§æ‡§†‡§≤‡§æ‡§à ‡§≠‡•á‡§ï‡•ç‡§ü‡§∞ (‡§á‡§Æ‡•ç‡§¨‡•á‡§°‡§ø‡§ô) ‡§Æ‡§æ ‡§∞‡•Ç‡§™‡§æ‡§®‡•ç‡§§‡§∞‡§£ ‡§ó‡§∞‡•ç‡§õ‡•§"""
    try:
        # Using the stable embedding-001 model as per user feedback
        result = genai.embed_content(
            model="models/embedding-001",
            content=text,
            task_type="retrieval_document"
        )
        return result['embedding']
    except ResourceExhausted as e:
        print("Embedding quota error:", e)
        log_exception(e)
        # Return a specific error code or message
        return "QUOTA_EXCEEDED"
    except Exception as e:
        print("Embedding error:", e)
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text if len(text.strip()) >= 100 else None
    except Exception as e:
        print(f"PDF ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        log_exception(e)
        return None

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()
        
        model = genai.GenerativeModel('gemini-1.5-flash')
        img_file = genai.upload_file(img_path)
        response = model.generate_content(["Extract all text from this document page:", img_file])
        return response.text
    except ResourceExhausted as e:
        print(f"Vision OCR quota error: {e}")
        log_exception(e)
        return "QUOTA_EXCEEDED_VISION"
    except Exception as e:
        print(f"Vision OCR ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã: {e}")
        log_exception(e)
        return None
    finally:
        # FIX: Ensure temp image is always deleted
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def call_gemini_smart(prompt, history=[]):
    """Simplified smart call with model fallback."""
    if not GEMINI_API_KEY:
        return "SERVICE_ERROR: Gemini API key is not configured."
    for model_name in MODELS:
        try:
            model = genai.GenerativeModel(model_name)
            if history:
                chat = model.start_chat(history=history)
                response = chat.send_message(prompt)
            else:
                response = model.generate_content(prompt)
            return response.text
        except ResourceExhausted as e:
            print(f"Model {model_name} quota error: {e}")
            log_exception(e)
            return "‚ùå AI Quota Error: The daily free limit for AI responses has been reached. Please try again tomorrow."
        except Exception as e:
            print(f"Model {model_name} failed.")
            log_exception(e)
            time.sleep(1)
            continue
    return "‚ùå All AI models failed."


# ‚Äî‚Äî‚Äî BOT MESSAGE HANDLERS ‚Äî‚Äî‚Äî

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§ø‡§ú‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ (Knowledge Base) ‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å‡•§")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ ‡§ß‡•á‡§∞‡•à ‡§†‡•Ç‡§≤‡•ã ‡§õ (20MB+)‡•§")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"‡§Ø‡•ã PDF ‡§™‡§π‡§ø‡§≤‡•á ‡§®‡•à ‡§¨‡§ö‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ‡•§")

    status_msg = bot.send_message(message.chat.id, f"‚è≥ '{message.document.file_name}' ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡•à...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡§æ‡§† ‡§´‡•á‡§≤‡§æ ‡§™‡§∞‡•á‡§®, Vision OCR ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            if text == "QUOTA_EXCEEDED_VISION":
                return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing scanned documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("‚ùå ‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ø‡•ã PDF ‡§¨‡§æ‡§ü ‡§ï‡•Å‡§®‡•à ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"‡§Ø‡•ã ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä‡§≤‡§æ‡§à ‡§ñ‡•ã‡§ú ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§£‡§ø‡§ï‡§æ‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡•® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§Æ‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç: {text[:2000]}"
        summary = call_gemini_smart(summary_prompt)
        vector = get_embedding(summary)
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing new documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector: return bot.edit_message_text("‚ùå AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"‚úÖ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ó‡§∞‡§ø‡§Ø‡•ã‡•§", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§´‡§æ‡§á‡§≤ ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {e}", status_msg.chat.id, status_msg.message_id)
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "‚ùå ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡§£‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§® ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§õ‡•à‡§®‡•§ ‡§§‡§™‡§æ‡§à‡§Ç ‡§Ø‡§∏‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ AI ‡§∏‡§Å‡§ó ‡§∏‡•ã‡§ß‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "üõ°Ô∏è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ Private Message (PM) ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "üõ°Ô∏è ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ PM ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"üìÑ Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≠‡§Ø‡•ã: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")
    status_msg = bot.reply_to(message, "üîç ‡§´‡§æ‡§á‡§≤‡§π‡§∞‡•Ç‡§Æ‡§æ ‡§ñ‡•ã‡§ú‡•ç‡§¶‡•à...")
    try:
        vector = get_embedding(query)
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for asking questions has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector:
            return bot.edit_message_text("‚ùå AI Error: ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§≠‡•á‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§®‡§æ‡§â‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ü‡§´‡•ç‡§®‡•ã API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç ‡§ú‡§æ‡§Å‡§ö ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§", status_msg.chat.id, status_msg.message_id)
        
        pipeline = [{" $vectorSearch": {"index": "vector_index", "path": "embedding", "queryVector": vector, "numCandidates": 10, "limit": 1}}]
        results = list(pdf_collection.aggregate(pipeline))
        if not results: return bot.edit_message_text("‚ùå ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§", message.chat.id, status_msg.message_id)
        
        context = results[0]['summary']
        prompt = f"Context from PDF: {context}\n\nUser Question: {query}\n\nAnswer based on context only:"
        
        bot.edit_message_text("‚úçÔ∏è ‡§∏‡§æ‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§ø‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§Ø‡•ã, ‡§ú‡§µ‡§æ‡§´ ‡§§‡§Ø‡§æ‡§∞ ‡§™‡§æ‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
        ai_response = call_gemini_smart(prompt)

        bot.delete_message(message.chat.id, status_msg.message_id)
        send_long_message(message.chat.id, f"üìÑ **‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§Æ‡§æ ‡§ú‡§µ‡§æ‡§´:**\n\n{ai_response}", reply_to_message_id=message.message_id, parse_mode="Markdown")

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", status_msg.chat.id, status_msg.message_id)

@bot.message_handler(commands=['quiz'])
def generate_pdf_quiz(message):
    args = message.text.split()
    if len(args) < 2: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ PDF ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/quiz 1`")
    try:
        pdf_id = int(args[1])
        res = pdf_collection.find_one({"serial_number": pdf_id})
        if not res: return bot.reply_to(message, "‡§Ø‡•ã ‡§®‡§Æ‡•ç‡§¨‡§∞‡§ï‡•ã ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")

        status_msg = bot.reply_to(message, f"‚è≥ {res['file_name']} ‡§¨‡§æ‡§ü ‡§ï‡•ç‡§µ‡§ø‡§ú ‡§§‡§Ø‡§æ‡§∞ ‡§ó‡§∞‡•ç‡§¶‡•à‡§õ‡•Å...")
        prompt = f"Create 1 MCQ quiz in JSON based on this: {res['summary']}. Return only JSON."
        ai_res = call_gemini_smart(prompt)
        data = json.loads(clean_json(ai_res))
        bot.send_poll(message.chat.id, question=data['question'][:255], options=[o[:100] for o in data['options']], correct_option_id=data['correct_option_id'], type='quiz', explanation=data.get('explanation', '')[:200])
        bot.delete_message(message.chat.id, status_msg.message_id)
    except Exception as e: 
        log_exception(e)
        bot.edit_message_text(f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", message.chat.id, status_msg.message_id if 'status_msg' in locals() else message.message_id)

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        history = get_chat_history(message.from_user.id)
        bot.send_chat_action(message.chat.id, 'typing')
        res = call_gemini_smart(message.text, history)
        save_chat_history(message.from_user.id, message.text, res)
        # FIX: Use the correct variable 'res' instead of 'bot_response'
        send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# ‚Äî‚Äî‚Äî BOT START (Render Safe) ‚Äî‚Äî‚Äî
def run_bot():
    bot.infinity_polling(skip_pending=True, timeout=30, long_polling_timeout=30)

if __name__ == "__main__":
    print("Bot started...")
    threading.Thread(target=run_bot).start()
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 10000)))
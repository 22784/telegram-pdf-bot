import certifi
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import math
import traceback
import requests
from flask import Flask, request, jsonify
import threading
from PIL import Image

app = Flask(__name__)

@app.route("/")
def home():
    return "Bot is running"

# тАФтАФтАФ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди (Render Environment Variables рдмрд╛рдЯ рдкрдвреНрдиреЗ) тАФтАФтАФ
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # Simplified to a single key
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# тАФтАФтАФ INITIALIZATION (Render-optimized) тАФтАФтАФ
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# API Setup
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("WARNING: GEMINI_API_KEY missing")

# Working model list as of Jan 2026, supporting vision
WORKING_MODELS = [
    "models/gemini-2.5-flash",
    "models/gemini-2.5-flash-lite",
    "models/gemini-2.0-flash-lite",
]

# рдХреНрд╡реЛрдЯрд╛ рдЯреНрд░реИрдХрд┐рдВрдЧ рдХреЗ рд▓рд┐рдП
failed_models = set()

# тАФтАФтАФ CORE HELPER FUNCTIONS тАФтАФтАФ

def log_exception(e):
    """рд╡рд┐рд╕реНрддреГрдд рддреНрд░реБрдЯрд┐ рд▓рдЧ рдЧрд░реНрдирдХрд╛ рд▓рд╛рдЧрд┐ред"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def cosine_similarity(a, b):
    dot = sum(x*y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x*x for x in a))
    norm_b = math.sqrt(sum(y*y for y in b))
    # Handle potential zero division
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)

def get_embedding(text, task_type="retrieval_document"):
    try:
        # Use the new API for embedding
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type=task_type
        )
        return result['embedding']
    except ResourceExhausted:
        return "QUOTA_EXCEEDED"
    except Exception as e:
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_vision_text(file_path):
    """
    рдпреЛ рдлрд╕рдирд▓реЗ PDF рдХреЛ рдкрд╣рд┐рд▓реЛ рдкреЗрдЬрд▓рд╛рдИ рдлреЛрдЯреЛрдорд╛ рдмрджрд▓реНрдЫ рд░ Gemini Vision рд▓рд╛рдИ рдкрдард╛рдЙрдБрдЫред
    рдпрд╕рд▓реЗ рдиреЗрдкрд╛рд▓реА рдлрдиреНрдЯ рд░ рдЧрдгрд┐рддреАрдп рдлрд░реНрдореБрд▓рд╛рд╣рд░реВ (Math) рдПрдХрджрдо рд╕рд╣реА рдирд┐рдХрд╛рд▓реНрдЫред
    """
    img_path = f"{file_path}_temp.png"
    uploaded_file = None # Use a different name to avoid confusion
    try:
        # 1. PDF рд▓рд╛рдИ рдлреЛрдЯреЛрдорд╛ рдмрджрд▓реНрдиреЗ (Zoom рдЧрд░реЗрд░)
        doc = fitz.open(file_path)
        mat = fitz.Matrix(2, 2)
        pix = doc[0].get_pixmap(matrix=mat)
        doc.close()
        pix.save(img_path)

        # 2. Gemini рдорд╛ рдлреЛрдЯреЛ рдЕрдкрд▓реЛрдб рдЧрд░реНрдиреЗ рд░ ACTIVE рд╣реБрди рдкрд░реНрдЦрд┐рдиреЗ
        print("Uploading image to Gemini for OCR...")
        uploaded_file = genai.upload_file(path=img_path, display_name=os.path.basename(img_path))
        
        print(f"File uploaded: {uploaded_file.name}, State: {uploaded_file.state.name}")
        while uploaded_file.state.name == "PROCESSING":
            print("Waiting for file to be processed...")
            time.sleep(4) # Increased sleep time
            uploaded_file = genai.get_file(name=uploaded_file.name)
            print(f"File state: {uploaded_file.state.name}")
            
        if uploaded_file.state.name != "ACTIVE":
            print(f"Error: Uploaded file is not active. State: {uploaded_file.state.name}")
            return None

        # 3. рдлреЛрдЯреЛрдмрд╛рдЯ рдЯреЗрдХреНрд╕реНрдЯ рдорд╛рдЧреНрдиреЗ (Fallback logic рж╕рж╣)
        prompt_parts = ["Extract all text from this document page exactly as it is. Preserve Nepali text and Math formulas.", uploaded_file]
        
        for model_name in WORKING_MODELS:
            try:
                print(f"Trying vision model: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt_parts)
                # If we get a response, return it immediately
                return response.text
            except Exception as e:
                error_msg = str(e).lower()
                print(f"Vision model {model_name} failed: {error_msg}")
                # If model is not found, or quota is hit, or it's an invalid argument for this model, try the next one.
                if any(err in error_msg for err in ["404", "not found", "quota", "invalid argument"]):
                    continue
                else:
                    log_exception(e)
                    continue # Try next model even for other errors
        
        # If all models failed
        print("All vision models failed to extract text.")
        return None
        
    except Exception as e:
        print(f"Vision Error: {e}")
        log_exception(e)
        return None
        
    finally:
        # рдЯреЗрдореНрдкреЛрд░рд░реА рдлреЛрдЯреЛ рд░ рдЕрдкрд▓реЛрдб рдЧрд░рд┐рдПрдХреЛ рдлрд╛рдЗрд▓ рдбрд┐рд▓рд┐рдЯ рдЧрд░реНрдиреЗ
        if uploaded_file:
            print(f"Deleting uploaded file: {uploaded_file.name}")
            genai.delete_file(name=uploaded_file.name)
        if os.path.exists(img_path):
            os.remove(img_path)

def smart_pdf_extract(file_path):
    """
    рдпреЛ 'Smart' рдлрд╕рди рд╣реЛред рдкрд╣рд┐рд▓реЗ рдпрд╕рд▓реЗ рд╕рд╛рдорд╛рдиреНрдп рддрд░рд┐рдХрд╛рд▓реЗ рдЯреЗрдХреНрд╕реНрдЯ рдирд┐рдХрд╛рд▓реНрди рдЦреЛрдЬреНрдЫред
    рдпрджрд┐ рдЯреЗрдХреНрд╕реНрдЯ рдмреБрдЭрд┐рдПрди рд╡рд╛ рдПрдХрджрдо рдХрдо рдЖрдпреЛ (рдЬрд╕реНрддреИ рд╕реНрдХреНрдпрд╛рди рдЧрд░реЗрдХреЛ рдлрд╛рдЗрд▓),
    рддрдм рдпрд╕рд▓реЗ рдорд╛рдерд┐рдХреЛ 'extract_vision_text' рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдЫред
    """
    try:
        # рд╕рд╛рдорд╛рдиреНрдп рддрд░рд┐рдХрд╛ (рдЫрд┐рдЯреЛ рд╣реБрдиреНрдЫ)
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()

        # рдпрджрд┐ рдЯреЗрдХреНрд╕реНрдЯ релреж рдЕрдХреНрд╖рд░ рднрдиреНрджрд╛ рдХрдо рдЫ рд╡рд╛ рдЦрд╛рд▓реА рдЫ рднрдиреЗ -> Vision рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреЗ
        if len(text.strip()) < 50:
            print("Low quality text detected, switching to Vision OCR...")
            vision_text = extract_vision_text(file_path)
            if vision_text:
                return vision_text, "Vision OCR (Image)"
            else:
                print("Vision OCR also failed to extract text.")
                return text, "Vision OCR Failed"  # Return original text but with a failure status
        
        return text, "Digital Text"
    except Exception as e:
        print(f"Standard text extraction failed: {e}. Falling back to Vision OCR.")
        vision_text = extract_vision_text(file_path)
        if vision_text:
            return vision_text, "Fallback OCR"
        else:
            return None, "Extraction Failed"

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def fallback_to_alternative_api(prompt):
    """рдЕрдиреНрдп рдлреНрд░реА рдПрдкреАрдЖрдИ рдХрд╛ рдЙрдкрдпреЛрдЧ (OpenAI, HuggingFace, рдЖрджрд┐)"""
    import requests
    
    try:
        # HuggingFace Inference API (рдлреНрд░реА)
        
        # рд╡реИрдХрд▓реНрдкрд┐рдХ 1: HuggingFace Zephyr
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            hf_response = requests.post(
                "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta",
                headers={"Authorization": f"Bearer {hf_token}"},
                json={"inputs": prompt}
            )
            
            if hf_response.status_code == 200:
                generated_text = hf_response.json()[0]['generated_text']
                # Clean up prompt from response if present
                if generated_text.startswith(prompt):
                    return generated_text[len(prompt):].strip()
                return generated_text
                
        # рд╡реИрдХрд▓реНрдкрд┐рдХ 2: OpenRouter (рдлреНрд░реА рдореЙрдбрд▓)
        openrouter_key = os.getenv('OPENROUTER_KEY')
        if openrouter_key:
            openrouter_response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {openrouter_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "google/gemini-2.0-flash-lite:free", # Using a free model on OpenRouter
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            
            if openrouter_response.status_code == 200:
                return openrouter_response.json()['choices'][0]['message']['content']
                
    except Exception as e:
        print(f"рдлреЗрд▓рдмреИрдХ API рддреНрд░реБрдЯрд┐: {e}")
        log_exception(e)
    
    return "тЭМ рд╕рднреА AI рд╕реЗрд╡рд╛рдПрдВ рдЕрд╕реНрдерд╛рдИ рд░реВрдк рд╕реЗ рдЕрдиреБрдкрд▓рдмреНрдз рд╣реИрдВред рдХреГрдкрдпрд╛ рдХреБрдЫ рд╕рдордп рдмрд╛рдж рдкреНрд░рдпрд╛рд╕ рдХрд░реЗрдВред"

def call_gemini_smart_improved(prompt, history=None):
    """рдХреНрд╡реЛрдЯрд╛ рдореИрдиреЗрдЬрдореЗрдВрдЯ рдФрд░ рдлреЗрд▓рдмреИрдХ рдХреЗ рд╕рд╛рде рдЕрдкрдбреЗрдЯреЗрдб рдлрдВрдХреНрд╢рди"""
    if not GEMINI_API_KEY:
        return "рд╕реЗрд╡рд╛ рдЙрдкрд▓рдмреНрдз рдирд╣реАрдВ рд╣реИред"
    
    # рд╕рд┐рд╕реНрдЯрдо рдЗрдВрд╕реНрдЯреНрд░рдХреНрд╢рди рдЬреЛрдбрд╝реЗрдВ (рдиреЗрдкрд╛рд▓реА рдлреЛрдВрдЯ рдХреЗ рд▓рд┐рдП)
    system_instruction = """
    рдЖрдк рдПрдХ рдорд▓реНрдЯреАрд▓рд┐рдВрдЧреБрдЕрд▓ рдЕрд╕рд┐рд╕реНрдЯреЗрдВрдЯ рд╣реИрдВ рдЬреЛ рдиреЗрдкрд╛рд▓реА, рд╣рд┐рдВрджреА рдФрд░ рдЕрдВрдЧреНрд░реЗрдЬреА рд╕рднреА рднрд╛рд╖рд╛рдУрдВ рдХреЛ рд╕рдордЭрддреЗ рдФрд░ рдкреНрд░реЛрд╕реЗрд╕ рдХрд░рддреЗ рд╣реИрдВред
    рдЖрдкрдХрд╛ рдкреНрд░рд╛рдердорд┐рдХ рд▓рдХреНрд╖реНрдп рдиреЗрдкрд╛рд▓реА рдореЗрдВ рдЬрд╡рд╛рдм рджреЗрдирд╛ рд╣реИред рдЖрдк рдХрднреА-рдХрднреА рдЕрдВрдЧреНрд░реЗрдЬреА рдпрд╛ рд╣рд┐рдВрджреА рд╢рдмреНрджреЛрдВ рдХрд╛ рдЙрдкрдпреЛрдЧ рдХрд░ рд╕рдХрддреЗ рд╣реИрдВ, рд▓реЗрдХрд┐рди рд╕реБрдирд┐рд╢реНрдЪрд┐рдд рдХрд░реЗрдВ рдХрд┐ рдореБрдЦреНрдп рднрд╛рд╖рд╛ рдиреЗрдкрд╛рд▓реА рд╣реА рд╣реЛред
    рд╕рдВрдЦреНрдпрд╛рдУрдВ рдФрд░ рд╡рд┐рд╢реЗрд╖ рдХрд░реИрдХреНрдЯрд░реНрд╕ рдХреЛ рд╕рд╣реА рд╕реЗ рд╣реИрдВрдбрд▓ рдХрд░реЗрдВред
    """
    
    # Prepare contents with system instruction and history
    contents = []
    if system_instruction:
        contents.append({"role": "user", "parts": [{"text": system_instruction}]})
        contents.append({"role": "model", "parts": [{"text": "рдареАрдХ рд╣реИ, рдореИрдВ рддреИрдпрд╛рд░ рд╣реВрдБред"}]}
    
    if history:
        contents.extend(history)
    
    contents.append({"role": "user", "parts": [{"text": prompt}]})

    for model_name in WORKING_MODELS: # Use the new WORKING_MODELS list
        if model_name in failed_models:
            continue
            
        try:
            print(f"рдЯреНрд░рд╛рдЗрдВрдЧ рдореЙрдбрд▓: {model_name}")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(contents)
            if response and response.text:
                return response.text
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"рдореЙрдбрд▓ {model_name} рдлреЗрд▓реНрдб: {error_msg}")
            
            # рдХреНрд╡реЛрдЯрд╛ рдПрд░рд░ рдХреА рдкрд╣рдЪрд╛рди
            if "quota" in error_msg or "429" in error_msg or "resource exhausted" in error_msg:
                print(f"рдХреНрд╡реЛрдЯрд╛ рд╕рдорд╛рдкреНрдд: {model_name}")
                failed_models.add(model_name)
                continue  # рдЕрдЧрд▓реЗ рдореЙрдбрд▓ рдХреА рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдВ
            elif "not found" in error_msg or "invalid" in error_msg:
                print(f"рдореЙрдбрд▓ рдирд╣реАрдВ рдорд┐рд▓рд╛: {model_name}")
                failed_models.add(model_name)
                continue
            else:
                # рдЕрдиреНрдп рдПрд░рд░ - рдереЛрдбрд╝реА рджреЗрд░ рд░реБрдХ рдХрд░ рдХреЛрд╢рд┐рд╢ рдХрд░реЗрдВ
                time.sleep(2)
                log_exception(e)
                continue
    
    # рд╕рднреА рдореЙрдбрд▓ рдлреЗрд▓ рд╣реЛрдиреЗ рдкрд░
    return fallback_to_alternative_api(prompt)


# тАФтАФтАФ BOT MESSAGE HANDLERS тАФтАФтАФ

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "рдирдорд╕реНрддреЗ! рдо рддрдкрд╛рдИрдВрдХреЛ рдирд┐рдЬреА рдЬреНрдЮрд╛рди рдЖрдзрд╛рд░ (Knowledge Base) рдмреЛрдЯ рд╣реБрдБред")

@bot.message_handler(content_types=['document'])
def handle_pdf(message):
    if message.document.mime_type != 'application/pdf':
        return bot.reply_to(message, "рдХреГрдкрдпрд╛ PDF рдлрд╛рдЗрд▓ рдорд╛рддреНрд░ рдкрдард╛рдЙрдиреБрд╣реЛрд╕реНред")

    status_msg = bot.send_message(message.chat.id, "ЁЯУе рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рд░ рд╕реНрдХреНрдпрд╛рди рдЧрд░реНрджреИ...")
    
    file_info = bot.get_file(message.document.file_id)
    downloaded_file = bot.download_file(file_info.file_path)
    file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
    
    with open(file_path, 'wb') as f:
        f.write(downloaded_file)

    try:
        # рез. рдЯреЗрдХреНрд╕реНрдЯ рдирд┐рдХрд╛рд▓реНрдиреЗ
        text, method = smart_pdf_extract(file_path)
        
        if method in ["Vision OCR Failed", "Extraction Failed"] or not text or not text.strip():
            return bot.edit_message_text("тЭМ рдорд╛рдл рдЧрд░реНрдиреБрд╣реЛрд╕реН, рдпреЛ PDF рдЦрд╛рд▓реА рдЫ рд╡рд╛ рдкрдвреНрди рд╕рдХрд┐рдПрдиред", message.chat.id, status_msg.message_id)

        # реи. рдбрд┐рдмрдЧ рдореНрдпрд╛рд╕реЗрдЬ
        debug_msg = f"ЁЯФН **DEBUG: Extracted Content ({method})**\n\n```\n{text[:800]}...\n```"
        bot.send_message(message.chat.id, debug_msg, parse_mode="Markdown")

        # рей. рд╕рд╛рд░рд╛рдВрд╢ рд░ Embedding
        summary_prompt = f"Summarize this in 3 sentences: {text[:4000]}"
        summary = call_gemini_smart_improved(summary_prompt)

        emb_result = genai.embed_content(
            model="models/text-embedding-004",
            content=summary,
            task_type="retrieval_document"
        )
        vector = emb_result['embedding']

        # рек. DB рдорд╛ рд╕реЗрдн (FIXED KEYS)
        serial = get_next_serial_number("pdf_id")
        pdf_collection.insert_one({
            "serial": serial,                     # Key: serial
            "file_name": message.document.file_name, # Key: file_name
            "file_id": message.document.file_id,     # NEW: file_id (for /get command)
            "text": text,
            "summary": summary,
            "embedding": vector,
            "uploader": message.from_user.id,
            "timestamp": time.time()
        })

        bot.edit_message_text(
            f"тЬЕ **PDF #{serial} рд╕реБрд░рдХреНрд╖рд┐рдд рднрдпреЛ!**\n\nЁЯУЭ **рд╕рд╛рд░рд╛рдВрд╢:**\n{summary}", 
            message.chat.id, status_msg.message_id, parse_mode="Markdown"
        )

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"тЭМ рддреНрд░реБрдЯрд┐ рдЖрдпреЛ: {str(e)}", message.chat.id, status_msg.message_id)
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "тЭМ Admin Only Command.")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "рдирдореНрдмрд░ рджрд┐рдиреБрд╣реЛрд╕реНред Ex: /get 1")
        
        index_no = int(args[1])
        
        # FIX: 'serial' рд╡рд╛ 'serial_number' рджреБрдмреИ рдЪреЗрдХ рдЧрд░реНрдиреЗ
        res = pdf_collection.find_one({"$or": [{"serial": index_no}, {"serial_number": index_no}]})
        
        if res:
            # FIX: file_id рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреЗ
            file_id = res.get('file_id')
            file_name = res.get('file_name', res.get('filename', 'document.pdf'))
            
            if file_id:
                bot.send_document(ADMIN_ID, file_id, caption=f"ЁЯУД Admin Copy: {file_name}")
            else:
                bot.reply_to(message, "тЭМ рдпреЛ рдлрд╛рдЗрд▓рдХреЛ ID рдбрд╛рдЯрд╛рдмреЗрд╕рдорд╛ рдЫреИрди (рд╕рд╛рдпрдж рдкреБрд░рд╛рдиреЛ рднрд░реНрд╕рдирдмрд╛рдЯ рдЕрдкрд▓реЛрдб рднрдПрдХреЛ рд╣реЛ)ред")
        else:
            bot.reply_to(message, "тЭМ рдлрд╛рдЗрд▓ рднреЗрдЯрд┐рдПрдиред")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"рддреНрд░реБрдЯрд┐ рднрдпреЛ: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query:
        return bot.reply_to(message, "рдХреГрдкрдпрд╛ рдлрд╛рдЗрд▓рдХреЛ рдмрд╛рд░реЗрдорд╛ рдХреЗрд╣реА рд╕реЛрдзреНрдиреБрд╣реЛрд╕реНред")
    
    status_msg = bot.reply_to(message, "ЁЯФН рдлрд╛рдЗрд▓рд╣рд░реВрдорд╛ рдЦреЛрдЬреНрджреИ рдЫреБ...")
    
    try:
        vector = get_embedding(query, task_type="RETRIEVAL_QUERY")
        
        # рез. рдбрд╛рдЯрд╛рдмреЗрд╕рдмрд╛рдЯ рдлрд╛рдЗрд▓рд╣рд░реВ рддрд╛рдиреНрдиреЗ
        all_pdfs = list(pdf_collection.find({}, {"serial": 1, "file_name": 1, "embedding": 1, "text": 1, "_id": 0}))
        
        if not all_pdfs:
            return ask_general_ai(message, query, status_msg)

        best_doc = None
        best_score = -1

        # реи. Similarity Search рдЧрд░реНрдиреЗ
        for doc in all_pdfs:
            if "embedding" in doc and doc["embedding"]:
                score = cosine_similarity(vector, doc["embedding"])
                if score > best_score:
                    best_score = score
                    best_doc = doc
        
        # рей. рд╕реНрдХреЛрд░ рдЪреЗрдХ рдЧрд░реНрдиреЗ (рд▓реБрдк рднрдиреНрджрд╛ рдмрд╛рд╣рд┐рд░)
        # рдпрджрд┐ рд╕реНрдХреЛрд░ рекреж% рднрдиреНрджрд╛ рдХрдо рдЫ рднрдиреЗ рд╕рд╛рдорд╛рдиреНрдп AI рд▓рд╛рдИ рд╕реЛрдзреНрдиреЗ
        if not best_doc or best_score < 0.40: 
            return ask_general_ai(message, query, status_msg)
        
        # рек. рдбреЗрдЯрд╛ рдирд┐рдХрд╛рд▓реНрджрд╛ .get() рдкреНрд░рдпреЛрдЧ рдЧрд░реНрдиреЗ (рдпрд╕рд▓реЗ KeyError рд░реЛрдХреНрдЫ)
        context = best_doc.get('text', '')[:3500]
        f_name = best_doc.get('file_name', 'Unknown File')
        f_serial = best_doc.get('serial', 'N/A')

        bot.edit_message_text(f"ЁЯУД **{f_name}** рдорд╛ рдЬрд╡рд╛рдл рднреЗрдЯрд┐рдпреЛ, рдкреНрд░реЛрд╕реЗрд╕ рдЧрд░реНрджреИрдЫреБ...", status_msg.chat.id, status_msg.message_id)
        
        prompt = f"Context from PDF (File: {f_name}):\n{context}\n\nQuestion: {query}\n\nAnswer in Nepali strictly based on the context provided."
        ai_response = call_gemini_smart_improved(prompt)
        
        bot.delete_message(message.chat.id, status_msg.message_id)
        send_long_message(
            message.chat.id,
            f"ЁЯУД **PDF #{f_serial} ({f_name}) рдХреЛ рдЖрдзрд╛рд░рдорд╛:**\n\n{ai_response}",
            reply_to_message_id=message.message_id
        )
            
    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"тЭМ рддреНрд░реБрдЯрд┐: {str(e)}", status_msg.chat.id, status_msg.message_id)

def ask_general_ai(message, query, status_msg=None):
    """рд╕рд╛рдорд╛рдиреНрдп AI рд╕реЗ рдЬрд╡рд╛рдм рд▓реЗрдВ"""
    if status_msg:
                    bot.edit_message_text(
                        "ЁЯдЦ AI рдмрд╛рдЯ рд╕рд╛рдорд╛рдиреНрдп рдЬреНрдЮрд╛рди рд▓рд┐рдБрджреИ рдЫреБ...",
                        status_msg.chat.id,
                        status_msg.message_id
                    )
    else:
        status_msg = bot.reply_to(message, "ЁЯдЦ AI рдмрд╛рдЯ рд╕рд╛рдорд╛рдиреНрдп рдЬреНрдЮрд╛рди рд▓рд┐рдБрджреИ рдЫреБ...")
    
    prompt = f"""
    User Question: {query}
    
    Instructions:
    1. Provide a helpful, accurate answer
    2. If you're not sure, say so
    3. Be concise but informative
    4. Answer in Hindi/English as appropriate. Prefer Nepali if context allows.
    
    Answer:
    """
    
    ai_response = call_gemini_smart_improved(prompt)
    
    bot.delete_message(message.chat.id, status_msg.message_id)
    send_long_message(
        message.chat.id,
        f"ЁЯдЦ **AI рдХреЛ рд╕рд╛рдорд╛рдиреНрдп рдЬрд╡рд╛рдл:**\n\n"
        f"{ai_response}\n\n"
        f"_ЁЯТб рдиреЛрдЯ: рдпреЛ рдЬрд╡рд╛рдл рдореЗрд░реЛ рд╕рд╛рдорд╛рдиреНрдп рдЬрд╛рдирдХрд╛рд░реАрдорд╛ рдЖрдзрд╛рд░рд┐рдд рдЫ, рдХреБрдиреИ рд╡рд┐рд╢реЗрд╖ рдлрд╛рдЗрд▓рдмрд╛рдЯ рд╣реЛрдЗрдиред_",
        reply_to_message_id=message.message_id,
        parse_mode="Markdown"
    )

@bot.message_handler(commands=['ask_ai'])
def ask_ai_command(message):
    query = message.text.replace('/ask_ai', '').strip()
    if not query:
        return bot.reply_to(message, "рдХреГрдкрдпрд╛ AI рдмрд╛рдЯ рдХреЗрд╣реА рд╕реЛрдзреНрдиреБрд╣реЛрд╕реНред")
    ask_general_ai(message, query)


@bot.message_handler(commands=['ask_smart'])
def ask_smart(message):
    """рд╕реНрдорд╛рд░реНрдЯрд▓реА рдбрд┐рд╕рд╛рдЗрдб рдХрд░реЗрдВ - PDF рдореЗрдВ рд╣реИ рдпрд╛ AI рд╕реЗ рдкреВрдЫреЗрдВ"""
    query = message.text.replace('/ask_smart', '').strip()
    
    if not query:
        return bot.reply_to(message, "рдкреНрд░рд╢реНрди рджрд░реНрдЬ рдХрд░реЗрдВред")
    
    # First, check if this is a PDF-related question
    pdf_keywords = ['pdf', 'file', 'document', 'рдлрд╛рдЗрд▓', 'рджрд╕реНрддрд╛рд╡реЗрдЬ', 'рдкреАрдбреАрдПрдл']
    
    is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
    
    if is_pdf_question:
        # Use /ask_file logic
        ask_from_file(message) # Note: this calls the modified ask_from_file
    else:
        # Direct AI chat
        handle_chat_improved(message) # This will be the new improved handler

@bot.message_handler(commands=['help'])
def help_command(message):
    help_text = """
    ЁЯдЦ **рдХрдорд╛рдВрдбреНрд╕:**
    
    `/ask_file [рдкреНрд░рд╢реНрди]` - рд╕рд┐рд░реНрдл PDFs рдореЗрдВ рдЦреЛрдЬреЗ (рдореИрдиреБрдЕрд▓ рд╡реЗрдХреНрдЯрд░ рдЦреЛрдЬ)
    `/ask_ai [рдкреНрд░рд╢реНрди]` - рд╕рд┐рд░реНрдл AI рд╕реЗ рдкреВрдЫреЗ (рд╕рд╛рдорд╛рдиреНрдп рдЬреНрдЮрд╛рди)
    `/ask_smart [рдкреНрд░рд╢реНрди]` - рдкрд╣рд▓реЗ PDF рдореЗрдВ рдЦреЛрдЬреЗрдЧрд╛ рдЕрдЧрд░ рдХреАрд╡рд░реНрдб рдорд┐рд▓рддреЗ рд╣реИрдВ, рд╡рд░рдирд╛ AI рд╕реЗ рдкреВрдЫреЗрдЧрд╛
    `/quiz [PDF рдирдВрдмрд░]` - PDF рдкрд░ рдЖрдзрд╛рд░рд┐рдд рдХреНрд╡рд┐рдЬ рдмрдирд╛рдПрдЧрд╛
    `/start` - рдмреЙрдЯ рдХрд╛ рдкрд░рд┐рдЪрдп
    
    **рдЙрджрд╛рд╣рд░рдг:**
    `/ask_file рдЗрд╕ PDF рдореЗрдВ рдХреНрдпрд╛ рд╣реИ?`
    `/ask_ai рднрд╛рд░рдд рдХреА рд░рд╛рдЬрдзрд╛рдиреА рдХреНрдпрд╛ рд╣реИ?`
    `/ask_smart machine learning рдХреНрдпрд╛ рд╣реИ?`
    """
    bot.reply_to(message, help_text, parse_mode="Markdown")

# Modify handle_chat to incorporate the ask_smart logic
@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        # Incorporate ask_smart logic
        query = message.text.strip()
        pdf_keywords = ['pdf', 'file', 'document', 'рдлрд╛рдЗрд▓', 'рджрд╕реНрддрд╛рд╡реЗрдЬ', 'рдкреАрдбреАрдПрдл']
        is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
        
        if is_pdf_question:
            ask_from_file(message)
        else:
            # Original handle_chat logic for general AI
            history = get_chat_history(message.from_user.id)
            bot.send_chat_action(message.chat.id, 'typing')
            res = call_gemini_smart_improved(message.text, history)
            save_chat_history(message.from_user.id, message.text, res)
            send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# тАФтАФтАФ BOT & SERVER RUNNER тАФтАФтАФ

def run_polling():
    """Runs the bot's polling loop in a resilient way."""
    while True:
        try:
            print("ЁЯдЦ Bot Polling Started...")
            bot.infinity_polling(timeout=20, long_polling_timeout=20, skip_pending=True)
        except Exception as e:
            print(f"ЁЯТе Polling Crash: {e}")
            log_exception(e)
            print("Restarting polling in 5 seconds...")
            time.sleep(5)

# Start the bot polling in a background thread.
# This runs when the module is imported by Gunicorn.
print("тЬЕ Starting bot polling in a background thread...")
threading.Thread(target=run_polling, daemon=True).start()

# The if __name__ block is now only for local development.
# Gunicorn will not run this, but it will run the code above.
if __name__ == "__main__":
    # When running locally, Flask's dev server is used.
    port = int(os.environ.get("PORT", 10000))
    print(f"Starting Flask dev server on http://0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port)

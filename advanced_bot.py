import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.generativeai as genai
import fitz  # PyMuPDF
import os
import json
import re
import time
from flask import Flask
from threading import Thread

# ‚Äî‚Äî‚Äî ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® (Render Environment Variables ‡§¨‡§æ‡§ü ‡§™‡§¢‡•ç‡§®‡•á) ‚Äî‚Äî‚Äî
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
# ‡¶è‡¶ï‡¶æ‡¶ß‡¶ø‡¶ï API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç‡§≤‡§æ‡§à ‡§Ö‡§≤‡•ç‡§™‡§µ‡§ø‡§∞‡§æ‡§Æ (comma) ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§õ‡•Å‡§ü‡•ç‡§Ø‡§æ‡§è‡§∞ ‡§è‡§â‡§ü‡•à ‡§≤‡§æ‡§á‡§®‡§Æ‡§æ ‡§∞‡§æ‡§ñ‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
API_KEYS = [key.strip() for key in os.getenv("API_KEYS", "").split(',') if key.strip()]
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# ‚Äî‚Äî‚Äî INITIALIZATION ‚Äî‚Äî‚Äî
bot = telebot.TeleBot(BOT_TOKEN)
client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# genai.configure ‡§Ö‡§¨ call_gemini_smart ‡§≠‡§ø‡§§‡•ç‡§∞ ‡§π‡•ç‡§Ø‡§æ‡§®‡•ç‡§°‡§≤ ‡§ó‡§∞‡§ø‡§®‡•ç‡§õ
MODELS = ["gemini-1.5-flash", "gemini-1.5-pro"]

# ‚Äî‚Äî‚Äî RENDER KEEP-ALIVE SERVER ‚Äî‚Äî‚Äî
app = Flask('')
@app.route('/')
def home():
    return "I am alive!"

def run_server():
  app.run(host='0.0.0.0', port=8080)

def keep_alive():
    t = Thread(target=run_server)
    t.start()

# ‚Äî‚Äî‚Äî CORE HELPER FUNCTIONS ‚Äî‚Äî‚Äî
def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def get_embedding(text):
    try:
        return genai.embed_content(model="models/text-embedding-004", content=text, task_type="retrieval_document")['embedding']
    except Exception as e:
        print(f"‡§á‡§Æ‡•ç‡§¨‡•á‡§°‡§ø‡§ô ‡§¨‡§®‡§æ‡§â‡§Å‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text if len(text.strip()) >= 100 else None
    except Exception as e:
        print(f"PDF ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        return None

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()
        
        model = genai.GenerativeModel('gemini-1.5-flash')
        img_file = genai.upload_file(img_path)
        response = model.generate_content(["Extract all text from this document page:", img_file])
        return response.text
    except Exception as e:
        print(f"Vision OCR ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã: {e}")
        return None
    finally:
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    """‡§ü‡•á‡§≤‡•Ä‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ï‡•ã ‡§ï‡•ç‡§Ø‡§æ‡§∞‡•á‡§ï‡•ç‡§ü‡§∞ ‡§≤‡§ø‡§Æ‡§ø‡§ü‡§≤‡§æ‡§à ‡§π‡•ç‡§Ø‡§æ‡§®‡•ç‡§°‡§≤ ‡§ó‡§∞‡•ç‡§¶‡•à ‡§≤‡§æ‡§Æ‡•ã ‡§∏‡§®‡•ç‡§¶‡•á‡§∂‡§π‡§∞‡•Ç ‡§™‡§†‡§æ‡§â‡§Å‡§õ‡•§"""
    if not text:
        return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
            time.sleep(1) # Spam filtering ‡§∏‡•á ‡§¨‡§ö‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})
def call_gemini_smart(prompt, history=[]):
    """AI ‡§≤‡§æ‡§à ‡§ï‡§≤ ‡§ó‡§∞‡•ç‡§õ, ‡§´‡§≤‡§¨‡•ç‡§Ø‡§æ‡§ï ‡§§‡§∞‡•ç‡§ï‡§ï‡§æ ‡§∏‡§æ‡§• (‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä ‡§∞ ‡§Æ‡•ã‡§°‡§≤ ‡§¶‡•Å‡§µ‡•à‡§Æ‡§æ)‡•§"""
    if not API_KEYS:
        print("API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞ ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§õ‡•à‡§®‡§®‡•ç!")
        return "SERVICE_ERROR: API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞ ‡§ó‡§∞‡§ø‡§è‡§ï‡§æ ‡§õ‡•à‡§®‡§®‡•ç‡•§"

    for key in API_KEYS: # ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•Ä ‡§π‡•á‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
        genai.configure(api_key=key) 
        for model_name in MODELS: # ‡§§‡•ç‡§Ø‡§∏‡§™‡§õ‡§ø ‡§™‡•ç‡§∞‡§§‡•ç‡§Ø‡•á‡§ï ‡§Æ‡•ã‡§°‡§≤ ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•Ä ‡§π‡•á‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
            try:
                model = genai.GenerativeModel(model_name)
                if history:
                    chat = model.start_chat(history=history)
                    response = chat.send_message(prompt)
                else:
                    response = model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã: {key[:5]}... ‡§Æ‡•ã‡§°‡§≤: {model_name} ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}. ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§¶‡•à‡§õ‡•Å...")
                time.sleep(1) # Rate limit ‡§¨‡§æ‡§ü ‡§¨‡§ö‡•ç‡§® ‡§•‡•ã‡§∞‡•à ‡§™‡§∞‡•ç‡§ñ‡§®‡•Å‡§π‡•ã‡§∏‡•ç
                continue # ‡§Ö‡§∞‡•ç‡§ï‡•ã ‡§Æ‡•ã‡§°‡§≤ ‡§µ‡§æ ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç
    return "‡§≠‡§æ‡§à, ‡§Ö‡§π‡§ø‡§≤‡•á ‡§∏‡§¨‡•à AI ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç ‡§∞ ‡§Æ‡•ã‡§°‡§≤‡§π‡§∞‡•Ç ‡§µ‡•ç‡§Ø‡§∏‡•ç‡§§ ‡§õ‡§®‡•ç‡•§ ‡§ï‡•á‡§π‡•Ä ‡§¨‡•á‡§∞ ‡§™‡§õ‡§ø ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§"


# ‚Äî‚Äî‚Äî BOT MESSAGE HANDLERS ‚Äî‚Äî‚Äî

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§ø‡§ú‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ (Knowledge Base) ‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å‡•§")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ ‡§ß‡•á‡§∞‡•à ‡§†‡•Ç‡§≤‡•ã ‡§õ (20MB+)‡•§")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"‡§Ø‡•ã PDF ‡§™‡§π‡§ø‡§≤‡•á ‡§®‡•à ‡§¨‡§ö‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ‡•§")

    status_msg = bot.send_message(message.chat.id, f"‚è≥ '{message.document.file_name}' ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡•à...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡§æ‡§† ‡§´‡•á‡§≤‡§æ ‡§™‡§∞‡•á‡§®, Vision OCR ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("‚ùå ‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ø‡•ã PDF ‡§¨‡§æ‡§ü ‡§ï‡•Å‡§®‡•à ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"‡§Ø‡•ã ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä‡§≤‡§æ‡§à ‡§ñ‡•ã‡§ú ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§£‡§ø‡§ï‡§æ‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡•® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§Æ‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç: {text[:2000]}"
        summary = call_gemini_smart(summary_prompt)
        vector = get_embedding(summary)
        if not vector: return bot.edit_message_text("‚ùå AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"‚úÖ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ó‡§∞‡§ø‡§Ø‡•ã‡•§", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        bot.edit_message_text(f"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§´‡§æ‡§á‡§≤ ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {e}", status_msg.chat.id, status_msg.message_id)
        print(f"Universal handler error: {e}")
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "‚ùå ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡§£‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§® ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§õ‡•à‡§®‡•§ ‡§§‡§™‡§æ‡§à‡§Ç ‡§Ø‡§∏‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ AI ‡§∏‡§Å‡§ó ‡§∏‡•ã‡§ß‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "üõ°Ô∏è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ Private Message (PM) ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "üõ°Ô∏è ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ PM ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"üìÑ Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")
    except Exception as e: bot.reply_to(message, f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≠‡§Ø‡•ã: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")
    status_msg = bot.reply_to(message, "üîç ‡§´‡§æ‡§á‡§≤‡§π‡§∞‡•Ç‡§Æ‡§æ ‡§ñ‡•ã‡§ú‡•ç‡§¶‡•à...")
    try:
        vector = get_embedding(query)
        results = list(pdf_collection.aggregate([{"$vectorSearch": {"index": "vector_index", "path": "embedding", "queryVector": vector, "numCandidates": 10, "limit": 1}}]))
        if not results: return bot.edit_message_text("‚ùå ‡§∏‡§Æ‡•ç‡§¨‡§®‡•ç‡§ß‡§ø‡§§ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§", message.chat.id, status_msg.message_id)
        
        context = results[0]['summary']
        prompt = f"Context from PDF: {context}\n\nUser Question: {query}\n\nAnswer based on context only:"
        ai_response = call_gemini_smart(prompt)
        bot.delete_message(message.chat.id, status_msg.message_id) # Delete status message
        send_long_message(message.chat.id, f"üìÑ **‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§Æ‡§æ ‡§ú‡§µ‡§æ‡§´:**\n\n{ai_response}", reply_to_message_id=message.message_id, parse_mode="Markdown")
    except Exception as e: bot.edit_message_text(f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", message.chat.id, status_msg.message_id)

@bot.message_handler(commands=['quiz'])
def generate_pdf_quiz(message):
    args = message.text.split()
    if len(args) < 2: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ PDF ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/quiz 1`")
    try:
        pdf_id = int(args[1])
        res = pdf_collection.find_one({"serial_number": pdf_id})
        if not res: return bot.reply_to(message, "‡§Ø‡•ã ‡§®‡§Æ‡•ç‡§¨‡§∞‡§ï‡•ã ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")

        status_msg = bot.reply_to(message, f"‚è≥ {res['file_name']} ‡§¨‡§æ‡§ü ‡§ï‡•ç‡§µ‡§ø‡§ú ‡§§‡§Ø‡§æ‡§∞ ‡§ó‡§∞‡•ç‡§¶‡•à‡§õ‡•Å...")
        prompt = f"Create 1 MCQ quiz in JSON based on this: {res['summary']}. Return only JSON."
        ai_res = call_gemini_smart(prompt)
        data = json.loads(clean_json(ai_res))
        bot.send_poll(message.chat.id, question=data['question'][:255], options=[o[:100] for o in data['options']], correct_option_id=data['correct_option_id'], type='quiz', explanation=data.get('explanation', '')[:200])
        bot.delete_message(message.chat.id, status_msg.message_id)
    except Exception as e: bot.edit_message_text(f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", message.chat.id, status_msg.message_id if 'status_msg' in locals() else message.message_id)

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        history = get_chat_history(message.from_user.id)
        bot.send_chat_action(message.chat.id, 'typing')
        res = call_gemini_smart(message.text, history)
        save_chat_history(message.from_user.id, message.text, res)
        send_long_message(message.chat.id, bot_response, reply_to_message_id=message.message_id)

# --- BOT START ---
if __name__ == "__main__":
    keep_alive()
    print("‡§¨‡•ã‡§ü ‡§≤‡§æ‡§á‡§≠ ‡§≠‡§Ø‡•ã...")
    bot.infinity_polling(skip_pending=True)

import certifi
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.genai as genai
from google.api_core.exceptions import ResourceExhausted
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import math
import traceback

from flask import Flask
import threading

app = Flask(__name__)

@app.route("/")
def home():
    return "Bot is running"

# ‚Äî‚Äî‚Äî ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® (Render Environment Variables ‡§¨‡§æ‡§ü ‡§™‡§¢‡•ç‡§®‡•á) ‚Äî‚Äî‚Äî
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # Simplified to a single key
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# ‚Äî‚Äî‚Äî INITIALIZATION (Render-optimized) ‚Äî‚Äî‚Äî
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# Configure Gemini with the single API key
if GEMINI_API_KEY:
    from google.genai import Client
    genai_client = Client(api_key=GEMINI_API_KEY)
else:
    print("WARNING: GEMINI_API_KEY environment variable not set.")
    genai_client = None

MODELS = [
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "gemini-1.0-pro"
]

# ‚Äî‚Äî‚Äî CORE HELPER FUNCTIONS ‚Äî‚Äî‚Äî

def log_exception(e):
    """‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≤‡§ó ‡§ó‡§∞‡•ç‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø‡•§"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def cosine_similarity(a, b):
    dot = sum(x*y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x*x for x in a))
    norm_b = math.sqrt(sum(y*y for y in b))
    # Handle potential zero division
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)

def get_embedding(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        res = genai_client.models.embed_content(
            model="text-embedding-004",
            contents=text,
            config={
                "task_type": task_type,
            }
        )
        return res.embeddings[0].values
    except ResourceExhausted:
        return "QUOTA_EXCEEDED"
    except Exception as e:
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text if len(text.strip()) >= 100 else None
    except Exception as e:
        print(f"PDF ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        log_exception(e)
        return None

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()

        uploaded_file = genai_client.files.upload(img_path)
        response = genai_client.models.generate_content(
            model="gemini-1.5-flash",
            contents=[
                "Extract all text from this document page:",
                uploaded_file
            ]
        )
        genai_client.files.delete(name=uploaded_file.name)
        return response.text
    except ResourceExhausted as e:
        print(f"Vision OCR quota error: {e}")
        log_exception(e)
        return "QUOTA_EXCEEDED_VISION"
    except Exception as e:
        print(f"Vision OCR ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã: {e}")
        log_exception(e)
        return None
    finally:
        # FIX: Ensure temp image is always deleted
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def call_gemini_smart(prompt, history=None):
    if not genai_client:
        return "‚ùå Gemini API key missing."

    contents = []
    if history:
        contents.extend(history)
    contents.append({"role": "user", "parts": [prompt]})

    for model_name in MODELS:
        try:
            response = genai_client.models.generate_content(
                model=f"models/{model_name}",
                contents=contents
            )
            if response and response.text:
                return response.text
        except ResourceExhausted:
            return "‚ùå AI Quota exhausted. Try tomorrow."
        except Exception as e:
            print(f"‚ö†Ô∏è Model failed: {model_name} ‚Üí {e}")
            time.sleep(1)
            continue
    return "‚ùå All Gemini models failed."


# ‚Äî‚Äî‚Äî BOT MESSAGE HANDLERS ‚Äî‚Äî‚Äî

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§ø‡§ú‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ (Knowledge Base) ‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å‡•§")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ ‡§ß‡•á‡§∞‡•à ‡§†‡•Ç‡§≤‡•ã ‡§õ (20MB+)‡•§")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"‡§Ø‡•ã PDF ‡§™‡§π‡§ø‡§≤‡•á ‡§®‡•à ‡§¨‡§ö‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ‡•§")

    status_msg = bot.send_message(message.chat.id, f"‚è≥ '{message.document.file_name}' ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡•à...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡§æ‡§† ‡§´‡•á‡§≤‡§æ ‡§™‡§∞‡•á‡§®, Vision OCR ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            if text == "QUOTA_EXCEEDED_VISION":
                return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing scanned documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("‚ùå ‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ø‡•ã PDF ‡§¨‡§æ‡§ü ‡§ï‡•Å‡§®‡•à ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"‡§Ø‡•ã ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä‡§≤‡§æ‡§à ‡§ñ‡•ã‡§ú ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§£‡§ø‡§ï‡§æ‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡•® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§Æ‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç: {text[:2000]}"
        summary = call_gemini_smart(summary_prompt)
        vector = get_embedding(summary, task_type="RETRIEVAL_DOCUMENT")
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing new documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector: return bot.edit_message_text("‚ùå AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "full_text": text, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"‚úÖ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ó‡§∞‡§ø‡§Ø‡•ã‡•§", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§´‡§æ‡§á‡§≤ ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {e}", status_msg.chat.id, status_msg.message_id)
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "‚ùå ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡§£‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§® ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§õ‡•à‡§®‡•§ ‡§§‡§™‡§æ‡§à‡§Ç ‡§Ø‡§∏‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ AI ‡§∏‡§Å‡§ó ‡§∏‡•ã‡§ß‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "üõ°Ô∏è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ Private Message (PM) ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "üõ°Ô∏è ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ PM ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"üìÑ Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≠‡§Ø‡•ã: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")
    status_msg = bot.reply_to(message, "üîç ‡§´‡§æ‡§á‡§≤‡§π‡§∞‡•Ç‡§Æ‡§æ ‡§ñ‡•ã‡§ú‡•ç‡§¶‡•à...")
    try:
        vector = get_embedding(query, task_type="RETRIEVAL_QUERY")
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for asking questions has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector:
            return bot.edit_message_text("‚ùå AI Error: ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§™‡•ç‡§∞‡§∂‡•ç‡§®‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡§≠‡•á‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§®‡§æ‡§â‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§Ü‡§´‡•ç‡§®‡•ã API ‡§ï‡•Å‡§û‡•ç‡§ú‡•Ä‡§π‡§∞‡•Ç ‡§ú‡§æ‡§Å‡§ö ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§", status_msg.chat.id, status_msg.message_id)
        
        # Manual Similarity Search (Option 1 from user)
        all_pdfs = list(pdf_collection.find({}, {"summary": 1, "embedding": 1, "full_text": 1, "_id": 0}))
        
        if not all_pdfs:
            return bot.edit_message_text("‚ùå ‡§ï‡•Å‡§®‡•à ‡§™‡§®‡§ø PDF ‡§π‡§∞‡•Ç ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡§®‡•ç‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§™‡§π‡§ø‡§≤‡•á PDF ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§", message.chat.id, status_msg.message_id)

        best_doc = None
        best_score = -1

        for doc in all_pdfs:
            # Ensure the document has an embedding
            if "embedding" in doc and doc["embedding"]:
                score = cosine_similarity(vector, doc["embedding"])
                if score > best_score:
                    best_score = score
                    best_doc = doc
        
        if not best_doc or best_score < 0.65:
            return bot.edit_message_text(
                "‚ùå ‡§á‡§∏ ‡§∏‡§µ‡§æ‡§≤ ‡§∏‡•á ‡§∞‡§ø‡§≤‡•á‡§ü‡•á‡§° ‡§ï‡•ã‡§à strong content ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ‡•§",
                message.chat.id, status_msg.message_id
            )

        context = best_doc['full_text'] if 'full_text' in best_doc else best_doc['summary']
        # Apply full text limit to prevent Gemini overload
        context = context[:3000]
        prompt = f"Context from PDF: {context}\n\nUser Question: {query}\n\nAnswer based on context only:"
        
        bot.edit_message_text("‚úçÔ∏è ‡§∏‡§æ‡§®‡•ç‡§¶‡§∞‡•ç‡§≠‡§ø‡§ï ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§Ø‡•ã, ‡§ú‡§µ‡§æ‡§´ ‡§§‡§Ø‡§æ‡§∞ ‡§™‡§æ‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
        ai_response = call_gemini_smart(prompt)

        bot.delete_message(message.chat.id, status_msg.message_id)
        send_long_message(message.chat.id, f"üìÑ **‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§Ü‡§ß‡§æ‡§∞‡§Æ‡§æ ‡§ú‡§µ‡§æ‡§´:**\n\n{ai_response}", reply_to_message_id=message.message_id, parse_mode="Markdown")

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§∏‡•ã‡§ß‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {e}", status_msg.chat.id, status_msg.message_id)

@bot.message_handler(commands=['quiz'])
def generate_pdf_quiz(message):
    args = message.text.split()
    if len(args) < 2: return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ PDF ‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/quiz 1`")
    try:
        pdf_id = int(args[1])
        res = pdf_collection.find_one({"serial_number": pdf_id})
        if not res: return bot.reply_to(message, "‡§Ø‡•ã ‡§®‡§Æ‡•ç‡§¨‡§∞‡§ï‡•ã ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")

        status_msg = bot.reply_to(message, f"‚è≥ {res['file_name']} ‡§¨‡§æ‡§ü ‡§ï‡•ç‡§µ‡§ø‡§ú ‡§§‡§Ø‡§æ‡§∞ ‡§ó‡§∞‡•ç‡§¶‡•à‡§õ‡•Å...")
        prompt = f"Create 1 MCQ quiz in JSON based on this: {res['summary']}. Return only JSON."
        ai_res = call_gemini_smart(prompt)
        data = json.loads(clean_json(ai_res))
        bot.send_poll(message.chat.id, question=data['question'][:255], options=[o[:100] for o in data['options']], correct_option_id=data['correct_option_id'], type='quiz', explanation=data.get('explanation', '')[:200])
        bot.delete_message(message.chat.id, status_msg.message_id)
    except Exception as e: 
        log_exception(e)
        bot.edit_message_text(f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}", message.chat.id, status_msg.message_id if 'status_msg' in locals() else message.message_id)

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        history = get_chat_history(message.from_user.id)
        bot.send_chat_action(message.chat.id, 'typing')
        res = call_gemini_smart(message.text, history)
        save_chat_history(message.from_user.id, message.text, res)
        # FIX: Use the correct variable 'res' instead of 'bot_response'
        send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# ‚Äî‚Äî‚Äî BOT START (Render Safe) ‚Äî‚Äî‚Äî
def run_bot():
    bot.infinity_polling(skip_pending=True, timeout=30, long_polling_timeout=30)

if __name__ == "__main__":
    print("Bot started...")
    threading.Thread(target=run_bot).start()
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 10000)))

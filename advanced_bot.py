import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.generativeai as genai
import fitz  # PyMuPDF
import os
import json
import re
import time
from flask import Flask
from threading import Thread

# тАФтАФтАФ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди (рдЖрдлреНрдиреЛ рд╡рд┐рд╡рд░рдг рдпрд╣рд╛рдБ рднрд░реНрдиреБрд╣реЛрд╕реН) тАФтАФтАФ
BOT_TOKEN = "YOUR_TELEGRAM_BOT_TOKEN"
MONGO_URI = "YOUR_MONGO_URI"
API_KEYS = ["YOUR_GEMINI_API_KEY_1", "YOUR_GEMINI_API_KEY_2"] # рдпрд╣рд╛рдБ рдЕрдкрдиреА рд╕рднреА Gemini API Key рдбрд╛рд▓реЗрдВ
ADMIN_ID = 123456789  # рддрдкрд╛рдИрдВрдХреЛ Telegram User ID
BACKUP_CHANNEL_ID = -100xxxxxxxxxx # рддрдкрд╛рдИрдВрдХреЛ рдирд┐рдЬреА рдЪреНрдпрд╛рдирд▓рдХреЛ ID

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# тАФтАФтАФ INITIALIZATION тАФтАФтАФ
bot = telebot.TeleBot(BOT_TOKEN)
client = MongoClient(MONGO_URI, serverSelectionTimeoutMS=5000)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

genai.configure(api_key=GEMINI_API_KEY)
MODELS = ["gemini-1.5-flash", "gemini-1.5-pro"]

# тАФтАФтАФ RENDER KEEP-ALIVE SERVER тАФтАФтАФ
app = Flask('')
@app.route('/')
def home():
    return "I am alive!"

def run_server():
  app.run(host='0.0.0.0', port=8080)

def keep_alive():
    t = Thread(target=run_server)
    t.start()

# тАФтАФтАФ CORE HELPER FUNCTIONS тАФтАФтАФ
def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def get_embedding(text):
    try:
        return genai.embed_content(model="models/text-embedding-004", content=text, task_type="retrieval_document")['embedding']
    except Exception as e:
        print(f"рдЗрдореНрдмреЗрдбрд┐рдЩ рдмрдирд╛рдЙрдБрджрд╛ рддреНрд░реБрдЯрд┐: {e}")
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text if len(text.strip()) >= 100 else None
    except Exception as e:
        print(f"PDF рдкрд╛рда рдирд┐рдХрд╛рд▓реНрджрд╛ рддреНрд░реБрдЯрд┐: {e}")
        return None

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()
        
        model = genai.GenerativeModel('gemini-1.5-flash')
        img_file = genai.upload_file(img_path)
        response = model.generate_content(["Extract all text from this document page:", img_file])
        return response.text
    except Exception as e:
        print(f"Vision OCR рдЕрд╕рдлрд▓ рднрдпреЛ: {e}")
        return None
    finally:
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    """рдЯреЗрд▓реАрдЧреНрд░рд╛рдордХреЛ рдХреНрдпрд╛рд░реЗрдХреНрдЯрд░ рд▓рд┐рдорд┐рдЯрд▓рд╛рдИ рд╣реНрдпрд╛рдиреНрдбрд▓ рдЧрд░реНрджреИ рд▓рд╛рдореЛ рд╕рдиреНрджреЗрд╢рд╣рд░реВ рдкрдард╛рдЙрдБрдЫред"""
    if not text:
        return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
            time.sleep(1) # Spam filtering рд╕реЗ рдмрдЪрдиреЗ рдХреЗ рд▓рд┐рдП

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})
def call_gemini_smart(prompt, history=[]):
    """AI рд▓рд╛рдИ рдХрд▓ рдЧрд░реНрдЫ, рдлрд▓рдмреНрдпрд╛рдХ рддрд░реНрдХрдХрд╛ рд╕рд╛рде (рдХреБрдЮреНрдЬреА рд░ рдореЛрдбрд▓ рджреБрд╡реИрдорд╛)ред"""
    if not API_KEYS:
        print("API рдХреБрдЮреНрдЬреАрд╣рд░реВ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░рд┐рдПрдХрд╛ рдЫреИрдирдиреН!")
        return "SERVICE_ERROR: API рдХреБрдЮреНрдЬреАрд╣рд░реВ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░рд┐рдПрдХрд╛ рдЫреИрдирдиреНред"

    for key in API_KEYS: # рдкреНрд░рддреНрдпреЗрдХ рдХреБрдЮреНрдЬреА рдкреНрд░рдпреЛрдЧ рдЧрд░реА рд╣реЗрд░реНрдиреБрд╣реЛрд╕реН
        genai.configure(api_key=key) 
        for model_name in MODELS: # рддреНрдпрд╕рдкрдЫрд┐ рдкреНрд░рддреНрдпреЗрдХ рдореЛрдбрд▓ рдкреНрд░рдпреЛрдЧ рдЧрд░реА рд╣реЗрд░реНрдиреБрд╣реЛрд╕реН
            try:
                model = genai.GenerativeModel(model_name)
                if history:
                    chat = model.start_chat(history=history)
                    response = chat.send_message(prompt)
                else:
                    response = model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"рдХреБрдЮреНрдЬреА рдЕрд╕рдлрд▓ рднрдпреЛ: {key[:5]}... рдореЛрдбрд▓: {model_name} рддреНрд░реБрдЯрд┐: {e}. рдЕрд░реНрдХреЛ рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрджреИрдЫреБ...")
                time.sleep(1) # Rate limit рдмрд╛рдЯ рдмрдЪреНрди рдереЛрд░реИ рдкрд░реНрдЦрдиреБрд╣реЛрд╕реН
                continue # рдЕрд░реНрдХреЛ рдореЛрдбрд▓ рд╡рд╛ рдХреБрдЮреНрдЬреА рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреБрд╣реЛрд╕реН
    return "рднрд╛рдИ, рдЕрд╣рд┐рд▓реЗ рд╕рдмреИ AI рдХреБрдЮреНрдЬреАрд╣рд░реВ рд░ рдореЛрдбрд▓рд╣рд░реВ рд╡реНрдпрд╕реНрдд рдЫрдиреНред рдХреЗрд╣реА рдмреЗрд░ рдкрдЫрд┐ рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрдиреБрд╣реЛрд╕реНред"


# тАФтАФтАФ BOT MESSAGE HANDLERS тАФтАФтАФ

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "рдирдорд╕реНрддреЗ! рдо рддрдкрд╛рдИрдВрдХреЛ рдирд┐рдЬреА рдЬреНрдЮрд╛рди рдЖрдзрд╛рд░ (Knowledge Base) рдмреЛрдЯ рд╣реБрдБред")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "рдпреЛ рдлрд╛рдЗрд▓ рдзреЗрд░реИ рдареВрд▓реЛ рдЫ (20MB+)ред")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"рдпреЛ PDF рдкрд╣рд┐рд▓реЗ рдиреИ рдмрдЪрдд рдЧрд░рд┐рдПрдХреЛ рдЫред")

    status_msg = bot.send_message(message.chat.id, f"тП│ '{message.document.file_name}' рдкреНрд░рд╢реЛрдзрди рдЧрд░реНрджреИ...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"рдбрд┐рдЬрд┐рдЯрд▓ рдкрд╛рда рдлреЗрд▓рд╛ рдкрд░реЗрди, Vision OCR рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрджреИ...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("тЭМ рдорд╛рдл рдЧрд░реНрдиреБрд╣реЛрд╕реН, рдпреЛ PDF рдмрд╛рдЯ рдХреБрдиреИ рдкрд╛рда рдирд┐рдХрд╛рд▓реНрди рд╕рдХрд┐рдПрдиред", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"рдпреЛ рд╕рд╛рдордЧреНрд░реАрд▓рд╛рдИ рдЦреЛрдЬ рдЕрдиреБрдХреНрд░рдордгрд┐рдХрд╛рдХреЛ рд▓рд╛рдЧрд┐ реи рд╡рд╛рдХреНрдпрдорд╛ рд╕рд╛рд░рд╛рдВрд╢ рдЧрд░реНрдиреБрд╣реЛрд╕реН: {text[:2000]}"
        summary = call_gemini_smart(summary_prompt)
        vector = get_embedding(summary)
        if not vector: return bot.edit_message_text("тЭМ AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"тЬЕ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ рдкреНрд░рд╢реЛрдзрди рд░ рд╕реБрд░рдХреНрд╖рд┐рдд рдЧрд░рд┐рдпреЛред", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        bot.edit_message_text(f"рдорд╛рдл рдЧрд░реНрдиреБрд╣реЛрд╕реН, рдлрд╛рдЗрд▓ рдкреНрд░рд╢реЛрдзрди рдЧрд░реНрджрд╛ рддреНрд░реБрдЯрд┐ рдЖрдпреЛ: {e}", status_msg.chat.id, status_msg.message_id)
        print(f"Universal handler error: {e}")
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "тЭМ рд╕реБрд░рдХреНрд╖рд╛ рдХрд╛рд░рдгрд▓реЗ рдЧрд░реНрджрд╛ PDF рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдЧрд░реНрди рдЕрдиреБрдорддрд┐ рдЫреИрдиред рддрдкрд╛рдИрдВ рдпрд╕рдХреЛ рдмрд╛рд░реЗрдорд╛ AI рд╕рдБрдЧ рд╕реЛрдзреНрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫред")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "ЁЯЫбя╕П рд╕реБрд░рдХреНрд╖рд╛рдХрд╛ рд▓рд╛рдЧрд┐, рдо рддрдкрд╛рдИрдВрд▓рд╛рдИ рдпреЛ рдлрд╛рдЗрд▓ Private Message (PM) рдорд╛ рдкрдард╛рдЙрдБрджреИрдЫреБред")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "ЁЯЫбя╕П рдо рддрдкрд╛рдИрдВрд▓рд╛рдИ рдпреЛ рдлрд╛рдЗрд▓ PM рдорд╛ рдкрдард╛рдЙрдБрджреИрдЫреБред")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "рдирдореНрдмрд░ рджрд┐рдиреБрд╣реЛрд╕реНред Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"ЁЯУД Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "рдлрд╛рдЗрд▓ рднреЗрдЯрд┐рдПрдиред")
    except Exception as e: bot.reply_to(message, f"рддреНрд░реБрдЯрд┐ рднрдпреЛ: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query: return bot.reply_to(message, "рдХреГрдкрдпрд╛ рдлрд╛рдЗрд▓рдХреЛ рдмрд╛рд░реЗрдорд╛ рдХреЗрд╣реА рд╕реЛрдзреНрдиреБрд╣реЛрд╕реНред")
    status_msg = bot.reply_to(message, "ЁЯФН рдлрд╛рдЗрд▓рд╣рд░реВрдорд╛ рдЦреЛрдЬреНрджреИ...")
    try:
        vector = get_embedding(query)
        results = list(pdf_collection.aggregate([{"$vectorSearch": {"index": "vector_index", "path": "embedding", "queryVector": vector, "numCandidates": 10, "limit": 1}}]))
        if not results: return bot.edit_message_text("тЭМ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЬрд╛рдирдХрд╛рд░реА рднреЗрдЯрд┐рдПрдиред", message.chat.id, status_msg.message_id)
        
        context = results[0]['summary']
        prompt = f"Context from PDF: {context}\n\nUser Question: {query}\n\nAnswer based on context only:"
        ai_response = call_gemini_smart(prompt)
        bot.delete_message(message.chat.id, status_msg.message_id) # Delete status message
        send_long_message(message.chat.id, f"ЁЯУД **рдлрд╛рдЗрд▓рдХреЛ рдЖрдзрд╛рд░рдорд╛ рдЬрд╡рд╛рдл:**\n\n{ai_response}", reply_to_message_id=message.message_id, parse_mode="Markdown")
    except Exception as e: bot.edit_message_text(f"рддреНрд░реБрдЯрд┐: {e}", message.chat.id, status_msg.message_id)

@bot.message_handler(commands=['quiz'])
def generate_pdf_quiz(message):
    args = message.text.split()
    if len(args) < 2: return bot.reply_to(message, "рдХреГрдкрдпрд╛ PDF рдирдореНрдмрд░ рджрд┐рдиреБрд╣реЛрд╕реНред рдЙрджрд╛рд╣рд░рдг: `/quiz 1`")
    try:
        pdf_id = int(args[1])
        res = pdf_collection.find_one({"serial_number": pdf_id})
        if not res: return bot.reply_to(message, "рдпреЛ рдирдореНрдмрд░рдХреЛ рдлрд╛рдЗрд▓ рднреЗрдЯрд┐рдПрдиред")

        status_msg = bot.reply_to(message, f"тП│ {res['file_name']} рдмрд╛рдЯ рдХреНрд╡рд┐рдЬ рддрдпрд╛рд░ рдЧрд░реНрджреИрдЫреБ...")
        prompt = f"Create 1 MCQ quiz in JSON based on this: {res['summary']}. Return only JSON."
        ai_res = call_gemini_smart(prompt)
        data = json.loads(clean_json(ai_res))
        bot.send_poll(message.chat.id, question=data['question'][:255], options=[o[:100] for o in data['options']], correct_option_id=data['correct_option_id'], type='quiz', explanation=data.get('explanation', '')[:200])
        bot.delete_message(message.chat.id, status_msg.message_id)
    except Exception as e: bot.edit_message_text(f"рддреНрд░реБрдЯрд┐: {e}", message.chat.id, status_msg.message_id if 'status_msg' in locals() else message.message_id)

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        history = get_chat_history(message.from_user.id)
        bot.send_chat_action(message.chat.id, 'typing')
        res = call_gemini_smart(message.text, history)
        save_chat_history(message.from_user.id, message.text, res)
        send_long_message(message.chat.id, bot_response, reply_to_message_id=message.message_id)

# --- BOT START ---
if __name__ == "__main__":
    keep_alive()
    print("рдмреЛрдЯ рд▓рд╛рдЗрдн рднрдпреЛ...")
    bot.infinity_polling(skip_pending=True)

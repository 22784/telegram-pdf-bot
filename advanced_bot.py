import certifi
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.genai as genai
from google.api_core.exceptions import ResourceExhausted
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import math
import traceback
import requests
from flask import Flask, request, jsonify
import threading

app = Flask(__name__)

@app.route("/")
def home():
    return "Bot is running"

# ‚Äî‚Äî‚Äî ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® (Render Environment Variables ‡§¨‡§æ‡§ü ‡§™‡§¢‡•ç‡§®‡•á) ‚Äî‚Äî‚Äî
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # Simplified to a single key
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# ‚Äî‚Äî‚Äî INITIALIZATION (Render-optimized) ‚Äî‚Äî‚Äî
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# Configure Gemini with the single API key
if GEMINI_API_KEY:
    from google.genai import Client
    genai_client = Client(api_key=GEMINI_API_KEY)
else:
    print("WARNING: GEMINI_API_KEY environment variable not set.")
    genai_client = None

MODELS = [
    "gemini-1.5-flash",
    "gemini-1.5-pro",
    "gemini-1.0-pro"
]

# ‡§Ö‡§™‡§°‡•á‡§ü‡•á‡§° ‡§Æ‡•â‡§°‡§≤ ‡§≤‡§ø‡§∏‡•ç‡§ü (2026 ‡§ï‡•á ‡§Ö‡§®‡•Å‡§∏‡§æ‡§∞)
FREE_TIER_MODELS = [
    "gemini-2.5-flash",
    "gemini-2.5-flash-lite", 
    "gemini-2.5-flash-lite-preview-09-2025",
    "gemini-2.0-flash-lite",
    "gemini-1.5-flash",  # ‡§≤‡•á‡§ó‡•á‡§∏‡•Ä ‡§´‡•á‡§≤‡§¨‡•à‡§ï
]

# ‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è
failed_models = set()

# ‚Äî‚Äî‚Äî CORE HELPER FUNCTIONS ‚Äî‚Äî‚Äî

def log_exception(e):
    """‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≤‡§ó ‡§ó‡§∞‡•ç‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø‡•§"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def cosine_similarity(a, b):
    dot = sum(x*y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x*x for x in a))
    norm_b = math.sqrt(sum(y*y for y in b))
    # Handle potential zero division
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)

def get_embedding(text, task_type="RETRIEVAL_DOCUMENT"):
    try:
        res = genai_client.models.embed_content(
            model="text-embedding-004",
            contents=text,
            config={
                "task_type": task_type,
            }
        )
        return res.embeddings[0].values
    except ResourceExhausted:
        return "QUOTA_EXCEEDED"
    except Exception as e:
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            # ‡§Ø‡•Ç‡§®‡§ø‡§ï‡•ã‡§° ‡§î‡§∞ ‡§≤‡•á‡§Ü‡§â‡§ü ‡§Æ‡•ã‡§° ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó
            text += page.get_text("text", sort=True, flags=fitz.TEXT_DEHYPHENATE | fitz.TEXT_PRESERVE_WHITESPACE)
        doc.close()
        
        # ‡§Ø‡§¶‡§ø ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§ï‡§Æ ‡§π‡•à ‡§§‡•ã OCR ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó
        if len(text.strip()) < 100:
            return extract_vision_text(file_path)
        return text
    except Exception as e:
        print(f"PDF ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§ø‡§ï‡§æ‡§≤‡§®‡•á ‡§Æ‡•á‡§Ç ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        log_exception(e)
        return extract_vision_text(file_path)  # ‡§´‡•á‡§≤‡§¨‡•à‡§ï ‡§ï‡•á ‡§∞‡•Ç‡§™ ‡§Æ‡•á‡§Ç OCR

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()

        uploaded_file = genai_client.files.upload(img_path)
        response = genai_client.models.generate_content(
            model="gemini-1.5-flash",
            contents=[
                "Extract all text from this document page:",
                uploaded_file
            ]
        )
        genai_client.files.delete(name=uploaded_file.name)
        return response.text
    except ResourceExhausted as e:
        print(f"Vision OCR quota error: {e}")
        log_exception(e)
        return "QUOTA_EXCEEDED_VISION"
    except Exception as e:
        print(f"Vision OCR ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã: {e}")
        log_exception(e)
        return None
    finally:
        # FIX: Ensure temp image is always deleted
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def fallback_to_alternative_api(prompt):
    """‡§Ö‡§®‡•ç‡§Ø ‡§´‡•ç‡§∞‡•Ä ‡§è‡§™‡•Ä‡§Ü‡§à ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó (OpenAI, HuggingFace, ‡§Ü‡§¶‡§ø)"""
    import requests
    
    try:
        # HuggingFace Inference API (‡§´‡•ç‡§∞‡•Ä)
        
        # ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï 1: HuggingFace Zephyr
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            hf_response = requests.post(
                "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta",
                headers={"Authorization": f"Bearer {hf_token}"},
                json={"inputs": prompt}
            )
            
            if hf_response.status_code == 200:
                generated_text = hf_response.json()[0]['generated_text']
                # Clean up prompt from response if present
                if generated_text.startswith(prompt):
                    return generated_text[len(prompt):].strip()
                return generated_text
                
        # ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï 2: OpenRouter (‡§´‡•ç‡§∞‡•Ä ‡§Æ‡•â‡§°‡§≤)
        openrouter_key = os.getenv('OPENROUTER_KEY')
        if openrouter_key:
            openrouter_response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {openrouter_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "google/gemini-2.0-flash-lite:free", # Using a free model on OpenRouter
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            
            if openrouter_response.status_code == 200:
                return openrouter_response.json()['choices'][0]['message']['content']
                
    except Exception as e:
        print(f"‡§´‡•á‡§≤‡§¨‡•à‡§ï API ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        log_exception(e)
    
    return "‚ùå ‡§∏‡§≠‡•Ä AI ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç ‡§Ö‡§∏‡•ç‡§•‡§æ‡§à ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§Ö‡§®‡•Å‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡§Ç‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§ï‡•Å‡§õ ‡§∏‡§Æ‡§Ø ‡§¨‡§æ‡§¶ ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§"

def call_gemini_smart_improved(prompt, history=None):
    """‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§Æ‡•à‡§®‡•á‡§ú‡§Æ‡•á‡§Ç‡§ü ‡§î‡§∞ ‡§´‡•á‡§≤‡§¨‡•à‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§°‡•á‡§ü‡•á‡§° ‡§´‡§Ç‡§ï‡•ç‡§∂‡§®"""
    if not GEMINI_API_KEY:
        return "‡§∏‡•á‡§µ‡§æ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
    
    # ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§á‡§Ç‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§∂‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç (‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§´‡•ã‡§Ç‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è)
    system_instruction = """
    ‡§Ü‡§™ ‡§è‡§ï ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§≤‡§ø‡§Ç‡§ó‡•Å‡§Ö‡§≤ ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü ‡§π‡•à‡§Ç‡•§ 
    ‡§®‡•á‡§™‡§æ‡§≤‡•Ä, ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§î‡§∞ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§∏‡§≠‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡•á‡§Ç ‡§î‡§∞ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§
    ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§î‡§∞ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ï‡§∞‡•à‡§ï‡•ç‡§ü‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§∏‡§π‡•Ä ‡§∏‡•á ‡§π‡•à‡§Ç‡§°‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§
    """
    
    # Prepare contents with system instruction and history
    contents = []
    if system_instruction:
        contents.append({"role": "user", "parts": [{"text": system_instruction}]})
        contents.append({"role": "model", "parts": [{"text": "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§"}]})
    
    if history:
        contents.extend(history)
    
    contents.append({"role": "user", "parts": [{"text": prompt}]})

    for model_name in FREE_TIER_MODELS: # Use the new FREE_TIER_MODELS list
        if model_name in failed_models:
            continue
            
        try:
            print(f"‡§ü‡•ç‡§∞‡§æ‡§á‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤: {model_name}")
            response = genai_client.models.generate_content(
                model=f"models/{model_name}",
                contents=contents
            )
            if response and response.text:
                return response.text
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"‡§Æ‡•â‡§°‡§≤ {model_name} ‡§´‡•á‡§≤‡•ç‡§°: {error_msg}")
            
            # ‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§è‡§∞‡§∞ ‡§ï‡•Ä ‡§™‡§π‡§ö‡§æ‡§®
            if "quota" in error_msg or "429" in error_msg or "resource exhausted" in error_msg:
                print(f"‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§: {model_name}")
                failed_models.add(model_name)
                continue  # ‡§Ö‡§ó‡§≤‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç
            elif "not found" in error_msg or "invalid" in error_msg:
                print(f"‡§Æ‡•â‡§°‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ: {model_name}")
                failed_models.add(model_name)
                continue
            else:
                # ‡§Ö‡§®‡•ç‡§Ø ‡§è‡§∞‡§∞ - ‡§•‡•ã‡§°‡§º‡•Ä ‡§¶‡•á‡§∞ ‡§∞‡•Å‡§ï ‡§ï‡§∞ ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç
                time.sleep(2)
                log_exception(e)
                continue
    
    # ‡§∏‡§≠‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§´‡•á‡§≤ ‡§π‡•ã‡§®‡•á ‡§™‡§∞
    return fallback_to_alternative_api(prompt)


# ‚Äî‚Äî‚Äî BOT MESSAGE HANDLERS ‚Äî‚Äî‚Äî

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§ø‡§ú‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ (Knowledge Base) ‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å‡•§")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ ‡§ß‡•á‡§∞‡•à ‡§†‡•Ç‡§≤‡•ã ‡§õ (20MB+)‡•§")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"‡§Ø‡•ã PDF ‡§™‡§π‡§ø‡§≤‡•á ‡§®‡•à ‡§¨‡§ö‡§§ ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§õ‡•§")

    status_msg = bot.send_message(message.chat.id, f"‚è≥ '{message.document.file_name}' ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡•à...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"‡§°‡§ø‡§ú‡§ø‡§ü‡§≤ ‡§™‡§æ‡§† ‡§´‡•á‡§≤‡§æ ‡§™‡§∞‡•á‡§®, Vision OCR ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡•ç‡§¶‡•à...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            if text == "QUOTA_EXCEEDED_VISION":
                return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing scanned documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("‚ùå ‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ø‡•ã PDF ‡§¨‡§æ‡§ü ‡§ï‡•Å‡§®‡•à ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"‡§Ø‡•ã ‡§∏‡§æ‡§Æ‡§ó‡•ç‡§∞‡•Ä‡§≤‡§æ‡§à ‡§ñ‡•ã‡§ú ‡§Ö‡§®‡•Å‡§ï‡•ç‡§∞‡§Æ‡§£‡§ø‡§ï‡§æ‡§ï‡•ã ‡§≤‡§æ‡§ó‡§ø ‡•® ‡§µ‡§æ‡§ï‡•ç‡§Ø‡§Æ‡§æ ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç: {text[:2000]}"
        summary = call_gemini_smart_improved(summary_prompt)
        vector = get_embedding(summary, task_type="RETRIEVAL_DOCUMENT")
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for processing new documents has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector: return bot.edit_message_text("‚ùå AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "full_text": text, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"‚úÖ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' ‡§∏‡§´‡§≤‡§§‡§æ‡§™‡•Ç‡§∞‡•ç‡§µ‡§ï ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§∞ ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§ó‡§∞‡§ø‡§Ø‡•ã‡•§", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§´‡§æ‡§á‡§≤ ‡§™‡•ç‡§∞‡§∂‡•ã‡§ß‡§® ‡§ó‡§∞‡•ç‡§¶‡§æ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {e}", status_msg.chat.id, status_msg.message_id)
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "‚ùå ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡§£‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§® ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§õ‡•à‡§®‡•§ ‡§§‡§™‡§æ‡§à‡§Ç ‡§Ø‡§∏‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ AI ‡§∏‡§Å‡§ó ‡§∏‡•ã‡§ß‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "üõ°Ô∏è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ Private Message (PM) ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "üõ°Ô∏è ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ PM ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"üìÑ Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≠‡§Ø‡•ã: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query:
        return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/ask_file ‡§Ø‡•ã PDF ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§õ?`")
    
    status_msg = bot.reply_to(message, "üîç ‡§´‡§æ‡§á‡§≤‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...")
    
    try:
        # Step 1: Generate embedding for the query
        vector = get_embedding(query, task_type="RETRIEVAL_QUERY")
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for asking questions has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector:
            return bot.edit_message_text(
                "‚ùå AI ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡§æ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§∏‡§´‡§≤‡•§",
                status_msg.chat.id, 
                status_msg.message_id
            )
        
        # Manual Similarity Search (Option 1)
        all_pdfs = list(pdf_collection.find({}, {"serial_number": 1, "file_name": 1, "summary": 1, "embedding": 1, "full_text": 1, "_id": 0}))
        
        if not all_pdfs:
            bot.edit_message_text(
                "üì≠ ‡§ï‡•Å‡§®‡•à ‡§™‡§®‡§ø PDF ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´ ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                status_msg.chat.id, 
                status_msg.message_id
            )
            general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer to this question based on your general knowledge."
            ai_response = call_gemini_smart_improved(general_prompt)
            
            bot.delete_message(message.chat.id, status_msg.message_id)
            send_long_message(
                message.chat.id, 
                f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                f"{ai_response}\n\n"
                f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
            return

        best_doc = None
        best_score = -1

        for doc in all_pdfs:
            if "embedding" in doc and doc["embedding"]:
                score = cosine_similarity(vector, doc["embedding"])
                if score > best_score:
                    best_score = score
                    best_doc = doc
        
        # Step 2: ‡§Ö‡§ó‡§∞ ‡§ï‡•ã‡§à PDF ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ ‡§Ø‡§æ ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§ï‡§Æ ‡§π‡•à
                if not best_doc or best_score < 0.50: # 0.50 is the similarity threshold
                    bot.edit_message_text(
                        "üì≠ ‡§´‡§æ‡§á‡§≤‡§Æ‡§æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®, AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´ ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                        status_msg.chat.id,
                        status_msg.message_id
                    )
                    
                    general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer to this question based on your general knowledge."
                    ai_response = call_gemini_smart_improved(general_prompt)
                    
                    bot.delete_message(message.chat.id, status_msg.message_id)
                    send_long_message(
                        message.chat.id,
                        f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                        f"{ai_response}\n\n"
                        f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                        reply_to_message_id=message.message_id,
                        parse_mode="Markdown"
                    )
                    return
        
        # Step 3: PDF ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à - ‡§∏‡§¨‡§∏‡•á relevant PDF ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
        context = best_doc['full_text'] if 'full_text' in best_doc else best_doc['summary']
        context = context[:3000] # Limit context to prevent Gemini overload
        
        bot.edit_message_text(
            f"üìÑ **{best_doc['file_name']}** ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...",
            status_msg.chat.id, 
            status_msg.message_id
        )
        
        # Enhanced prompt with page finding logic
        enhanced_prompt = f"""
        PDF Context (Relevant section from {best_doc['file_name']}):
        {context}
        
        User Question: {query}
        
        Instructions:
        1. Answer based ONLY on the given PDF context above
        2. If information is found, mention that it's from the PDF and indicate the serial number of the PDF.
        3. If possible, estimate which page this information might be on (e.g., "beginning," "middle," or "end" of the document, or "page X" if an exact number can be inferred from context, though exact page numbers are not available).
        4. If information is NOT in the context, say clearly "‡§Ø‡§π ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä PDF ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä‡•§"
        5. Answer in a natural, helpful way. Ensure all responses are primarily in Nepali if possible.
        
        Answer:
        """
        
        ai_response = call_gemini_smart_improved(enhanced_prompt)
        
        # Step 5: Format the response
        pdf_info = f"PDF #{best_doc['serial_number']} ({best_doc['file_name']})"
        
        # Check if AI found the answer in PDF
        not_found_phrases = ["not found", "‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ", "‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à", "not in the pdf"] # Added Nepali phrase
        if any(phrase in ai_response.lower() for phrase in not_found_phrases):
            # Fallback to general AI answer
            bot.edit_message_text(
                "üì≠ PDF ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä, AI ‡§∏‡•á ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨ ‡§≤‡•á ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...",
                status_msg.chat.id, 
                status_msg.message_id
            )
            
            general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer based on your general knowledge. Answer in Nepali."
            ai_response = call_gemini_smart_improved(general_prompt)
            
            bot.delete_message(message.chat.id, status_msg.message_id)
            send_long_message(
                message.chat.id,
                f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                f"{ai_response}\n\n"
                f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
        else:
            # Found in PDF - show with PDF info
            bot.delete_message(message.chat.id, status_msg.message_id)
            
            # Try to extract page number from AI response
            # Modified regex to be more flexible and capture page number hints
            page_match = re.search(r'(‡§™‡•á‡§ú\s*\d+|beginning|middle|end)', ai_response.lower())
            page_info = ""
            if page_match:
                page_info = f" ({page_match.group(0)})" # Use the captured group directly
            
            send_long_message(
                message.chat.id,
                f"üìÑ **{pdf_info}{page_info} ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞:**\n\n"
                f"{ai_response}\n\n"
                f"_‚úÖ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä PDF ‡§∏‡•á ‡§≤‡•Ä ‡§ó‡§à ‡§π‡•à_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
            
    except Exception as e:
        log_exception(e)
        bot.edit_message_text(
            f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {str(e)[:100]}",
            status_msg.chat.id, 
            status_msg.message_id
        )

def ask_general_ai(message, query, status_msg=None):
    """‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø AI ‡§∏‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§≤‡•á‡§Ç"""
    if status_msg:
                    bot.edit_message_text(
                        "ü§ñ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                        status_msg.chat.id,
                        status_msg.message_id
                    )
    else:
        status_msg = bot.reply_to(message, "ü§ñ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...")
    
    prompt = f"""
    User Question: {query}
    
    Instructions:
    1. Provide a helpful, accurate answer
    2. If you're not sure, say so
    3. Be concise but informative
    4. Answer in Hindi/English as appropriate. Prefer Nepali if context allows.
    
    Answer:
    """
    
    ai_response = call_gemini_smart_improved(prompt)
    
    bot.delete_message(message.chat.id, status_msg.message_id)
    send_long_message(
        message.chat.id,
        f"ü§ñ **AI ‡§ï‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´:**\n\n"
        f"{ai_response}\n\n"
        f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
        reply_to_message_id=message.message_id,
        parse_mode="Markdown"
    )

@bot.message_handler(commands=['ask_ai'])
def ask_ai_command(message):
    query = message.text.replace('/ask_ai', '').strip()
    if not query:
        return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ AI ‡§¨‡§æ‡§ü ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")
    ask_general_ai(message, query)


@bot.message_handler(commands=['ask_smart'])
def ask_smart(message):
    """‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü‡§≤‡•Ä ‡§°‡§ø‡§∏‡§æ‡§á‡§° ‡§ï‡§∞‡•á‡§Ç - PDF ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ø‡§æ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§Ç"""
    query = message.text.replace('/ask_smart', '').strip()
    
    if not query:
        return bot.reply_to(message, "‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç‡•§")
    
    # First, check if this is a PDF-related question
    pdf_keywords = ['pdf', 'file', 'document', '‡§´‡§æ‡§á‡§≤', '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú', '‡§™‡•Ä‡§°‡•Ä‡§è‡§´']
    
    is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
    
    if is_pdf_question:
        # Use /ask_file logic
        ask_from_file(message) # Note: this calls the modified ask_from_file
    else:
        # Direct AI chat
        handle_chat_improved(message) # This will be the new improved handler

@bot.message_handler(commands=['help'])
def help_command(message):
    help_text = """
    ü§ñ **‡§ï‡§Æ‡§æ‡§Ç‡§°‡•ç‡§∏:**
    
    `/ask_file [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§∏‡§ø‡§∞‡•ç‡§´ PDFs ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú‡•á (‡§Æ‡•à‡§®‡•Å‡§Ö‡§≤ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞ ‡§ñ‡•ã‡§ú)
    `/ask_ai [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§∏‡§ø‡§∞‡•ç‡§´ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á (‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§®)
    `/ask_smart [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§™‡§π‡§≤‡•á PDF ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú‡•á‡§ó‡§æ ‡§Ö‡§ó‡§∞ ‡§ï‡•Ä‡§µ‡§∞‡•ç‡§° ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç, ‡§µ‡§∞‡§®‡§æ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§ó‡§æ
    `/quiz [PDF ‡§®‡§Ç‡§¨‡§∞]` - PDF ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡•ç‡§µ‡§ø‡§ú ‡§¨‡§®‡§æ‡§è‡§ó‡§æ
    `/start` - ‡§¨‡•â‡§ü ‡§ï‡§æ ‡§™‡§∞‡§ø‡§ö‡§Ø
    
    **‡§â‡§¶‡§æ‡§π‡§∞‡§£:**
    `/ask_file ‡§á‡§∏ PDF ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    `/ask_ai ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    `/ask_smart machine learning ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    """
    bot.reply_to(message, help_text, parse_mode="Markdown")

# Modify handle_chat to incorporate the ask_smart logic
@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        # Incorporate ask_smart logic
        query = message.text.strip()
        pdf_keywords = ['pdf', 'file', 'document', '‡§´‡§æ‡§á‡§≤', '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú', '‡§™‡•Ä‡§°‡•Ä‡§è‡§´']
        is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
        
        if is_pdf_question:
            ask_from_file(message)
        else:
            # Original handle_chat logic for general AI
            history = get_chat_history(message.from_user.id)
            bot.send_chat_action(message.chat.id, 'typing')
            res = call_gemini_smart_improved(message.text, history)
            save_chat_history(message.from_user.id, message.text, res)
            send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# ‚Äî‚Äî‚Äî BOT START (Render Safe) ‚Äî‚Äî‚Äî
def run_bot():
    bot.infinity_polling(skip_pending=True, timeout=30, long_polling_timeout=30)

if __name__ == "__main__":
    print("Bot started...")
    threading.Thread(target=run_bot).start()
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 10000)))
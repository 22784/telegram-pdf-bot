import certifi
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.generativeai as genai
from google.api_core.exceptions import ResourceExhausted
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import math
import traceback
import requests
from flask import Flask, request, jsonify
import threading
from PIL import Image

app = Flask(__name__)

@app.route("/")
def home():
    return "Bot is running"

# ‚Äî‚Äî‚Äî ‡§ï‡§®‡•ç‡§´‡§ø‡§ó‡§∞‡•á‡§∏‡§® (Render Environment Variables ‡§¨‡§æ‡§ü ‡§™‡§¢‡•ç‡§®‡•á) ‚Äî‚Äî‚Äî
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # Simplified to a single key
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# ‚Äî‚Äî‚Äî INITIALIZATION (Render-optimized) ‚Äî‚Äî‚Äî
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

# API Setup
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
else:
    print("WARNING: GEMINI_API_KEY missing")

# Working model list as of Jan 2026, supporting vision
WORKING_MODELS = [
    "models/gemini-2.5-flash",
    "models/gemini-2.5-flash-lite",
    "models/gemini-2.0-flash-lite",
]

# ‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§ü‡•ç‡§∞‡•à‡§ï‡§ø‡§Ç‡§ó ‡§ï‡•á ‡§≤‡§ø‡§è
failed_models = set()

# ‚Äî‚Äî‚Äî CORE HELPER FUNCTIONS ‚Äî‚Äî‚Äî

def log_exception(e):
    """‡§µ‡§ø‡§∏‡•ç‡§§‡•É‡§§ ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≤‡§ó ‡§ó‡§∞‡•ç‡§®‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø‡•§"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def cosine_similarity(a, b):
    dot = sum(x*y for x, y in zip(a, b))
    norm_a = math.sqrt(sum(x*x for x in a))
    norm_b = math.sqrt(sum(y*y for y in b))
    # Handle potential zero division
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot / (norm_a * norm_b)

def get_embedding(text, task_type="retrieval_document"):
    try:
        # Use the new API for embedding
        result = genai.embed_content(
            model="models/text-embedding-004",
            content=text,
            task_type=task_type
        )
        return result['embedding']
    except ResourceExhausted:
        return "QUOTA_EXCEEDED"
    except Exception as e:
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_vision_text(file_path):
    """
    ‡§Ø‡•ã ‡§´‡§∏‡§®‡§≤‡•á PDF ‡§ï‡•ã ‡§™‡§π‡§ø‡§≤‡•ã ‡§™‡•á‡§ú‡§≤‡§æ‡§à ‡§´‡•ã‡§ü‡•ã‡§Æ‡§æ ‡§¨‡§¶‡§≤‡•ç‡§õ ‡§∞ Gemini Vision ‡§≤‡§æ‡§à ‡§™‡§†‡§æ‡§â‡§Å‡§õ‡•§
    ‡§Ø‡§∏‡§≤‡•á ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§´‡§®‡•ç‡§ü ‡§∞ ‡§ó‡§£‡§ø‡§§‡•Ä‡§Ø ‡§´‡§∞‡•ç‡§Æ‡•Å‡§≤‡§æ‡§π‡§∞‡•Ç (Math) ‡§è‡§ï‡§¶‡§Æ ‡§∏‡§π‡•Ä ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§õ‡•§
    """
    img_path = f"{file_path}_temp.png"
    uploaded_file = None # Use a different name to avoid confusion
    try:
        # 1. PDF ‡§≤‡§æ‡§à ‡§´‡•ã‡§ü‡•ã‡§Æ‡§æ ‡§¨‡§¶‡§≤‡•ç‡§®‡•á (Zoom ‡§ó‡§∞‡•á‡§∞)
        doc = fitz.open(file_path)
        mat = fitz.Matrix(2, 2)
        pix = doc[0].get_pixmap(matrix=mat)
        doc.close()
        pix.save(img_path)

        # 2. Gemini ‡§Æ‡§æ ‡§´‡•ã‡§ü‡•ã ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§®‡•á ‡§∞ ACTIVE ‡§π‡•Å‡§® ‡§™‡§∞‡•ç‡§ñ‡§ø‡§®‡•á
        print("Uploading image to Gemini for OCR...")
        uploaded_file = genai.upload_file(path=img_path, display_name=os.path.basename(img_path))
        
        print(f"File uploaded: {uploaded_file.name}, State: {uploaded_file.state.name}")
        while uploaded_file.state.name == "PROCESSING":
            print("Waiting for file to be processed...")
            time.sleep(4) # Increased sleep time
            uploaded_file = genai.get_file(name=uploaded_file.name)
            print(f"File state: {uploaded_file.state.name}")
            
        if uploaded_file.state.name != "ACTIVE":
            print(f"Error: Uploaded file is not active. State: {uploaded_file.state.name}")
            return None

        # 3. ‡§´‡•ã‡§ü‡•ã‡§¨‡§æ‡§ü ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§Æ‡§æ‡§ó‡•ç‡§®‡•á (Fallback logic ‡¶∏‡¶π)
        prompt_parts = ["Extract all text from this document page exactly as it is. Preserve Nepali text and Math formulas.", uploaded_file]
        
        for model_name in WORKING_MODELS:
            try:
                print(f"Trying vision model: {model_name}")
                model = genai.GenerativeModel(model_name)
                response = model.generate_content(prompt_parts)
                # If we get a response, return it immediately
                return response.text
            except Exception as e:
                error_msg = str(e).lower()
                print(f"Vision model {model_name} failed: {error_msg}")
                # If model is not found, or quota is hit, or it's an invalid argument for this model, try the next one.
                if any(err in error_msg for err in ["404", "not found", "quota", "invalid argument"]):
                    continue
                else:
                    log_exception(e)
                    continue # Try next model even for other errors
        
        # If all models failed
        print("All vision models failed to extract text.")
        return None
        
    except Exception as e:
        print(f"Vision Error: {e}")
        log_exception(e)
        return None
        
    finally:
        # ‡§ü‡•á‡§Æ‡•ç‡§™‡•ã‡§∞‡§∞‡•Ä ‡§´‡•ã‡§ü‡•ã ‡§∞ ‡§Ö‡§™‡§≤‡•ã‡§° ‡§ó‡§∞‡§ø‡§è‡§ï‡•ã ‡§´‡§æ‡§á‡§≤ ‡§°‡§ø‡§≤‡§ø‡§ü ‡§ó‡§∞‡•ç‡§®‡•á
        if uploaded_file:
            print(f"Deleting uploaded file: {uploaded_file.name}")
            genai.delete_file(name=uploaded_file.name)
        if os.path.exists(img_path):
            os.remove(img_path)

def smart_pdf_extract(file_path):
    """
    ‡§Ø‡•ã 'Smart' ‡§´‡§∏‡§® ‡§π‡•ã‡•§ ‡§™‡§π‡§ø‡§≤‡•á ‡§Ø‡§∏‡§≤‡•á ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§§‡§∞‡§ø‡§ï‡§æ‡§≤‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§ñ‡•ã‡§ú‡•ç‡§õ‡•§
    ‡§Ø‡§¶‡§ø ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§¨‡•Å‡§ù‡§ø‡§è‡§® ‡§µ‡§æ ‡§è‡§ï‡§¶‡§Æ ‡§ï‡§Æ ‡§Ü‡§Ø‡•ã (‡§ú‡§∏‡•ç‡§§‡•à ‡§∏‡•ç‡§ï‡•ç‡§Ø‡§æ‡§® ‡§ó‡§∞‡•á‡§ï‡•ã ‡§´‡§æ‡§á‡§≤),
    ‡§§‡§¨ ‡§Ø‡§∏‡§≤‡•á ‡§Æ‡§æ‡§•‡§ø‡§ï‡•ã 'extract_vision_text' ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§õ‡•§
    """
    try:
        # ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§§‡§∞‡§ø‡§ï‡§æ (‡§õ‡§ø‡§ü‡•ã ‡§π‡•Å‡§®‡•ç‡§õ)
        doc = fitz.open(file_path)
        text = ""
        for page in doc:
            text += page.get_text()
        doc.close()

        # ‡§Ø‡§¶‡§ø ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡•´‡•¶ ‡§Ö‡§ï‡•ç‡§∑‡§∞ ‡§≠‡§®‡•ç‡§¶‡§æ ‡§ï‡§Æ ‡§õ ‡§µ‡§æ ‡§ñ‡§æ‡§≤‡•Ä ‡§õ ‡§≠‡§®‡•á -> Vision ‡§™‡•ç‡§∞‡§Ø‡•ã‡§ó ‡§ó‡§∞‡•ç‡§®‡•á
        if len(text.strip()) < 50:
            print("Low quality text detected, switching to Vision OCR...")
            vision_text = extract_vision_text(file_path)
            if vision_text:
                return vision_text, "Vision OCR (Image)"
            else:
                print("Vision OCR also failed to extract text.")
                return text, "Vision OCR Failed"  # Return original text but with a failure status
        
        return text, "Digital Text"
    except Exception as e:
        print(f"Standard text extraction failed: {e}. Falling back to Vision OCR.")
        vision_text = extract_vision_text(file_path)
        if vision_text:
            return vision_text, "Fallback OCR"
        else:
            return None, "Extraction Failed"

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def fallback_to_alternative_api(prompt):
    """‡§Ö‡§®‡•ç‡§Ø ‡§´‡•ç‡§∞‡•Ä ‡§è‡§™‡•Ä‡§Ü‡§à ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó (OpenAI, HuggingFace, ‡§Ü‡§¶‡§ø)"""
    import requests
    
    try:
        # HuggingFace Inference API (‡§´‡•ç‡§∞‡•Ä)
        
        # ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï 1: HuggingFace Zephyr
        hf_token = os.getenv('HF_TOKEN')
        if hf_token:
            hf_response = requests.post(
                "https://api-inference.huggingface.co/models/HuggingFaceH4/zephyr-7b-beta",
                headers={"Authorization": f"Bearer {hf_token}"},
                json={"inputs": prompt}
            )
            
            if hf_response.status_code == 200:
                generated_text = hf_response.json()[0]['generated_text']
                # Clean up prompt from response if present
                if generated_text.startswith(prompt):
                    return generated_text[len(prompt):].strip()
                return generated_text
                
        # ‡§µ‡•à‡§ï‡§≤‡•ç‡§™‡§ø‡§ï 2: OpenRouter (‡§´‡•ç‡§∞‡•Ä ‡§Æ‡•â‡§°‡§≤)
        openrouter_key = os.getenv('OPENROUTER_KEY')
        if openrouter_key:
            openrouter_response = requests.post(
                "https://openrouter.ai/api/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {openrouter_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "model": "google/gemini-2.0-flash-lite:free", # Using a free model on OpenRouter
                    "messages": [{"role": "user", "content": prompt}]
                }
            )
            
            if openrouter_response.status_code == 200:
                return openrouter_response.json()['choices'][0]['message']['content']
                
    except Exception as e:
        print(f"‡§´‡•á‡§≤‡§¨‡•à‡§ï API ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {e}")
        log_exception(e)
    
    return "‚ùå ‡§∏‡§≠‡•Ä AI ‡§∏‡•á‡§µ‡§æ‡§è‡§Ç ‡§Ö‡§∏‡•ç‡§•‡§æ‡§à ‡§∞‡•Ç‡§™ ‡§∏‡•á ‡§Ö‡§®‡•Å‡§™‡§≤‡§¨‡•ç‡§ß ‡§π‡•à‡§Ç‡•§ ‡§ï‡•É‡§™‡§Ø‡§æ ‡§ï‡•Å‡§õ ‡§∏‡§Æ‡§Ø ‡§¨‡§æ‡§¶ ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ï‡§∞‡•á‡§Ç‡•§"

def call_gemini_smart_improved(prompt, history=None):
    """‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§Æ‡•à‡§®‡•á‡§ú‡§Æ‡•á‡§Ç‡§ü ‡§î‡§∞ ‡§´‡•á‡§≤‡§¨‡•à‡§ï ‡§ï‡•á ‡§∏‡§æ‡§• ‡§Ö‡§™‡§°‡•á‡§ü‡•á‡§° ‡§´‡§Ç‡§ï‡•ç‡§∂‡§®"""
    if not GEMINI_API_KEY:
        return "‡§∏‡•á‡§µ‡§æ ‡§â‡§™‡§≤‡§¨‡•ç‡§ß ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à‡•§"
    
    # ‡§∏‡§ø‡§∏‡•ç‡§ü‡§Æ ‡§á‡§Ç‡§∏‡•ç‡§ü‡•ç‡§∞‡§ï‡•ç‡§∂‡§® ‡§ú‡•ã‡§°‡§º‡•á‡§Ç (‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§´‡•ã‡§Ç‡§ü ‡§ï‡•á ‡§≤‡§ø‡§è)
    system_instruction = """
    ‡§Ü‡§™ ‡§è‡§ï ‡§Æ‡§≤‡•ç‡§ü‡•Ä‡§≤‡§ø‡§Ç‡§ó‡•Å‡§Ö‡§≤ ‡§Ö‡§∏‡§ø‡§∏‡•ç‡§ü‡•á‡§Ç‡§ü ‡§π‡•à‡§Ç ‡§ú‡•ã ‡§®‡•á‡§™‡§æ‡§≤‡•Ä, ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§î‡§∞ ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§∏‡§≠‡•Ä ‡§≠‡§æ‡§∑‡§æ‡§ì‡§Ç ‡§ï‡•ã ‡§∏‡§Æ‡§ù‡§§‡•á ‡§î‡§∞ ‡§™‡•ç‡§∞‡•ã‡§∏‡•á‡§∏ ‡§ï‡§∞‡§§‡•á ‡§π‡•à‡§Ç‡•§
    ‡§Ü‡§™‡§ï‡§æ ‡§™‡•ç‡§∞‡§æ‡§•‡§Æ‡§ø‡§ï ‡§≤‡§ï‡•ç‡§∑‡•ç‡§Ø ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§Æ‡•á‡§Ç ‡§ú‡§µ‡§æ‡§¨ ‡§¶‡•á‡§®‡§æ ‡§π‡•à‡•§ ‡§Ü‡§™ ‡§ï‡§≠‡•Ä-‡§ï‡§≠‡•Ä ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Ø‡§æ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞ ‡§∏‡§ï‡§§‡•á ‡§π‡•à‡§Ç, ‡§≤‡•á‡§ï‡§ø‡§® ‡§∏‡•Å‡§®‡§ø‡§∂‡•ç‡§ö‡§ø‡§§ ‡§ï‡§∞‡•á‡§Ç ‡§ï‡§ø ‡§Æ‡•Å‡§ñ‡•ç‡§Ø ‡§≠‡§æ‡§∑‡§æ ‡§®‡•á‡§™‡§æ‡§≤‡•Ä ‡§π‡•Ä ‡§π‡•ã‡•§
    ‡§∏‡§Ç‡§ñ‡•ç‡§Ø‡§æ‡§ì‡§Ç ‡§î‡§∞ ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§ï‡§∞‡•à‡§ï‡•ç‡§ü‡§∞‡•ç‡§∏ ‡§ï‡•ã ‡§∏‡§π‡•Ä ‡§∏‡•á ‡§π‡•à‡§Ç‡§°‡§≤ ‡§ï‡§∞‡•á‡§Ç‡•§
    """
    
    # Prepare contents with system instruction and history
    contents = []
    if system_instruction:
        contents.append({"role": "user", "parts": [{"text": system_instruction}]})
        contents.append({"role": "model", "parts": [{"text": "‡§†‡•Ä‡§ï ‡§π‡•à, ‡§Æ‡•à‡§Ç ‡§§‡•à‡§Ø‡§æ‡§∞ ‡§π‡•Ç‡§Å‡•§"}]})
    
    if history:
        contents.extend(history)
    
    contents.append({"role": "user", "parts": [{"text": prompt}]})

    for model_name in WORKING_MODELS: # Use the new WORKING_MODELS list
        if model_name in failed_models:
            continue
            
        try:
            print(f"‡§ü‡•ç‡§∞‡§æ‡§á‡§Ç‡§ó ‡§Æ‡•â‡§°‡§≤: {model_name}")
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(contents)
            if response and response.text:
                return response.text
            
        except Exception as e:
            error_msg = str(e).lower()
            print(f"‡§Æ‡•â‡§°‡§≤ {model_name} ‡§´‡•á‡§≤‡•ç‡§°: {error_msg}")
            
            # ‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§è‡§∞‡§∞ ‡§ï‡•Ä ‡§™‡§π‡§ö‡§æ‡§®
            if "quota" in error_msg or "429" in error_msg or "resource exhausted" in error_msg:
                print(f"‡§ï‡•ç‡§µ‡•ã‡§ü‡§æ ‡§∏‡§Æ‡§æ‡§™‡•ç‡§§: {model_name}")
                failed_models.add(model_name)
                continue  # ‡§Ö‡§ó‡§≤‡•á ‡§Æ‡•â‡§°‡§≤ ‡§ï‡•Ä ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç
            elif "not found" in error_msg or "invalid" in error_msg:
                print(f"‡§Æ‡•â‡§°‡§≤ ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ: {model_name}")
                failed_models.add(model_name)
                continue
            else:
                # ‡§Ö‡§®‡•ç‡§Ø ‡§è‡§∞‡§∞ - ‡§•‡•ã‡§°‡§º‡•Ä ‡§¶‡•á‡§∞ ‡§∞‡•Å‡§ï ‡§ï‡§∞ ‡§ï‡•ã‡§∂‡§ø‡§∂ ‡§ï‡§∞‡•á‡§Ç
                time.sleep(2)
                log_exception(e)
                continue
    
    # ‡§∏‡§≠‡•Ä ‡§Æ‡•â‡§°‡§≤ ‡§´‡•á‡§≤ ‡§π‡•ã‡§®‡•á ‡§™‡§∞
    return fallback_to_alternative_api(prompt)


# ‚Äî‚Äî‚Äî BOT MESSAGE HANDLERS ‚Äî‚Äî‚Äî

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "‡§®‡§Æ‡§∏‡•ç‡§§‡•á! ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§ï‡•ã ‡§®‡§ø‡§ú‡•Ä ‡§ú‡•ç‡§û‡§æ‡§® ‡§Ü‡§ß‡§æ‡§∞ (Knowledge Base) ‡§¨‡•ã‡§ü ‡§π‡•Å‡§Å‡•§")

@bot.message_handler(content_types=['document'])
def handle_pdf(message):
    if message.document.mime_type != 'application/pdf':
        return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§Æ‡§æ‡§§‡•ç‡§∞ ‡§™‡§†‡§æ‡§â‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")

    status_msg = bot.send_message(message.chat.id, "üì• ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§∞ ‡§∏‡•ç‡§ï‡•ç‡§Ø‡§æ‡§® ‡§ó‡§∞‡•ç‡§¶‡•à...")
    
    # ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§°
    file_info = bot.get_file(message.document.file_id)
    downloaded_file = bot.download_file(file_info.file_path)
    file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
    
    with open(file_path, 'wb') as f:
        f.write(downloaded_file)

    try:
        # ‡•ß. ‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü ‡§§‡§∞‡§ø‡§ï‡§æ‡§≤‡•á ‡§ü‡•á‡§ï‡•ç‡§∏‡•ç‡§ü ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§®‡•á (‡§®‡§Ø‡§æ‡§Å ‡§ï‡•ã‡§°)
        text, method = smart_pdf_extract(file_path)
        
        # Improved error handling based on the method
        if method in ["Vision OCR Failed", "Extraction Failed"] or not text or not text.strip():
            error_msg = "‚ùå ‡§Æ‡§æ‡§´ ‡§ó‡§∞‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç, ‡§Ø‡•ã PDF ‡§¨‡§æ‡§ü ‡§ï‡•Å‡§®‡•à ‡§™‡§æ‡§† ‡§®‡§ø‡§ï‡§æ‡§≤‡•ç‡§® ‡§∏‡§ï‡§ø‡§è‡§®‡•§"
            if method == "Vision OCR Failed":
                error_msg += "\n\n(AI Vision ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡§®‡§ø ‡§™‡•ç‡§∞‡§Ø‡§æ‡§∏ ‡§ó‡§∞‡§ø‡§Ø‡•ã ‡§§‡§∞ ‡§Ö‡§∏‡§´‡§≤ ‡§≠‡§Ø‡•ã‡•§)" # Also tried with AI Vision but it failed.
            return bot.edit_message_text(error_msg, message.chat.id, status_msg.message_id, parse_mode="Markdown")

        # ‡•®. ‡§°‡§ø‡§¨‡§ó‡§ø‡§ô (‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡•á ‡§Æ‡§æ‡§ó‡•ç‡§®‡•Å‡§≠‡§è‡§ï‡•ã ‡§´‡§ø‡§ö‡§∞): ‡§¨‡•ã‡§ü‡§≤‡•á ‡§ï‡•á ‡§™‡§¢‡•ç‡§Ø‡•ã ‡§≠‡§®‡•á‡§∞ ‡§π‡•á‡§∞‡•ç‡§®‡•á
        # ‡§Ø‡•ã ‡§™‡§õ‡§ø ‡§π‡§ü‡§æ‡§â‡§® ‡§∏‡§ï‡§ø‡§®‡•ç‡§õ
        debug_msg = f"üîç **DEBUG: Extracted Content ({method})**\n\n```\n{text[:800]}...\n```"
        bot.send_message(message.chat.id, debug_msg, parse_mode="Markdown")

        # ‡•©. ‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂ ‡§∞ ‡§∏‡•á‡§≠ ‡§ó‡§∞‡•ç‡§®‡•á (Fallback logic ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá)
        summary_prompt = f"Summarize this in 3 sentences: {text[:4000]}"
        summary = call_gemini_smart_improved(summary_prompt)

        if not summary or "All AI services are temporarily unavailable" in summary:
            return bot.edit_message_text("‚ùå AI Error: Could not generate a summary for the document.", message.chat.id, status_msg.message_id)
        
        
        # Embedding (‡§®‡§Ø‡§æ‡§Å ‡§§‡§∞‡§ø‡§ï‡§æ)
        emb_result = genai.embed_content(
            model="models/text-embedding-004",
            content=summary,
            task_type="retrieval_document"
        )
        vector = emb_result['embedding']

        # DB ‡§Æ‡§æ ‡§∏‡•á‡§≠
        serial = get_next_serial_number("pdf_id")
        pdf_collection.insert_one({
            "serial": serial,
            "file_name": message.document.file_name,
            "text": text,
            "summary": summary,
            "embedding": vector,
            "uploader": message.from_user.id
        })

        bot.edit_message_text(
            f"‚úÖ **PDF #{serial} ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§ø‡§§ ‡§≠‡§Ø‡•ã!**\n\nüìù **‡§∏‡§æ‡§∞‡§æ‡§Ç‡§∂:**\n{summary}", 
            message.chat.id, status_msg.message_id, parse_mode="Markdown"
        )

    except Exception as e:
        log_exception(e) # Use the logging helper
        bot.edit_message_text(f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§Ü‡§Ø‡•ã: {str(e)}", message.chat.id, status_msg.message_id)
    finally:
        if os.path.exists(file_path):
            os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "‚ùå ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ ‡§ï‡§æ‡§∞‡§£‡§≤‡•á ‡§ó‡§∞‡•ç‡§¶‡§æ PDF ‡§´‡§æ‡§á‡§≤ ‡§°‡§æ‡§â‡§®‡§≤‡•ã‡§° ‡§ó‡§∞‡•ç‡§® ‡§Ö‡§®‡•Å‡§Æ‡§§‡§ø ‡§õ‡•à‡§®‡•§ ‡§§‡§™‡§æ‡§à‡§Ç ‡§Ø‡§∏‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ AI ‡§∏‡§Å‡§ó ‡§∏‡•ã‡§ß‡•ç‡§® ‡§∏‡§ï‡•ç‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ‡•§")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "üõ°Ô∏è ‡§∏‡•Å‡§∞‡§ï‡•ç‡§∑‡§æ‡§ï‡§æ ‡§≤‡§æ‡§ó‡§ø, ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ Private Message (PM) ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "üõ°Ô∏è ‡§Æ ‡§§‡§™‡§æ‡§à‡§Ç‡§≤‡§æ‡§à ‡§Ø‡•ã ‡§´‡§æ‡§á‡§≤ PM ‡§Æ‡§æ ‡§™‡§†‡§æ‡§â‡§Å‡§¶‡•à‡§õ‡•Å‡•§")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "‡§®‡§Æ‡•ç‡§¨‡§∞ ‡§¶‡§ø‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"üìÑ Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"‡§§‡•ç‡§∞‡•Å‡§ü‡§ø ‡§≠‡§Ø‡•ã: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query:
        return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ ‡§´‡§æ‡§á‡§≤‡§ï‡•ã ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: `/ask_file ‡§Ø‡•ã PDF ‡§ï‡•á ‡§¨‡§æ‡§∞‡•á‡§Æ‡§æ ‡§õ?`")
    
    status_msg = bot.reply_to(message, "üîç ‡§´‡§æ‡§á‡§≤‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...")
    
    try:
        # Step 1: Generate embedding for the query
        vector = get_embedding(query, task_type="RETRIEVAL_QUERY")
        if vector == "QUOTA_EXCEEDED":
            return bot.edit_message_text("‚ùå AI Quota Error: The daily free limit for asking questions has been reached. Please try again tomorrow.", status_msg.chat.id, status_msg.message_id)
        if not vector:
            return bot.edit_message_text(
                "‚ùå AI ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: ‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§ï‡§æ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞ ‡§¨‡§®‡§æ‡§®‡•á ‡§Æ‡•á‡§Ç ‡§Ö‡§∏‡§´‡§≤‡•§",
                status_msg.chat.id, 
                status_msg.message_id
            )
        
        # Manual Similarity Search (Option 1)
        all_pdfs = list(pdf_collection.find({}, {"serial_number": 1, "file_name": 1, "summary": 1, "embedding": 1, "full_text": 1, "_id": 0}))
        
        if not all_pdfs:
            bot.edit_message_text(
                "üì≠ ‡§ï‡•Å‡§®‡•à ‡§™‡§®‡§ø PDF ‡§´‡§æ‡§á‡§≤ ‡§≠‡•á‡§ü‡§ø‡§è‡§®‡•§ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´ ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                status_msg.chat.id, 
                status_msg.message_id
            )
            general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer to this question based on your general knowledge."
            ai_response = call_gemini_smart_improved(general_prompt)
            
            bot.delete_message(message.chat.id, status_msg.message_id)
            send_long_message(
                message.chat.id, 
                f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                f"{ai_response}\n\n"
                f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
            return

        best_doc = None
        best_score = -1

        for doc in all_pdfs:
            if "embedding" in doc and doc["embedding"]:
                score = cosine_similarity(vector, doc["embedding"])
                if score > best_score:
                    best_score = score
                    best_doc = doc
        
        # Step 2: ‡§Ö‡§ó‡§∞ ‡§ï‡•ã‡§à PDF ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ ‡§Ø‡§æ ‡§∏‡•ç‡§ï‡•ã‡§∞ ‡§ï‡§Æ ‡§π‡•à
                if not best_doc or best_score < 0.50: # 0.50 is the similarity threshold
                    bot.edit_message_text(
                        "üì≠ ‡§´‡§æ‡§á‡§≤‡§Æ‡§æ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§≠‡•á‡§ü‡§ø‡§è‡§®, AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´ ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                        status_msg.chat.id,
                        status_msg.message_id
                    )
                    
                    general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer to this question based on your general knowledge."
                    ai_response = call_gemini_smart_improved(general_prompt)
                    
                    bot.delete_message(message.chat.id, status_msg.message_id)
                    send_long_message(
                        message.chat.id,
                        f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                        f"{ai_response}\n\n"
                        f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                        reply_to_message_id=message.message_id,
                        parse_mode="Markdown"
                    )
                    return
        
        # Step 3: PDF ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à - ‡§∏‡§¨‡§∏‡•á relevant PDF ‡§ï‡§æ ‡§â‡§™‡§Ø‡•ã‡§ó ‡§ï‡§∞‡•á‡§Ç
        context = best_doc['full_text'] if 'full_text' in best_doc else best_doc['summary']
        context = context[:3000] # Limit context to prevent Gemini overload
        
        bot.edit_message_text(
            f"üìÑ **{best_doc['file_name']}** ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...",
            status_msg.chat.id, 
            status_msg.message_id
        )
        
        # Enhanced prompt with page finding logic
        enhanced_prompt = f"""
        PDF Context (Relevant section from {best_doc['file_name']}):
        {context}
        
        User Question: {query}
        
        Instructions:
        1. Answer based ONLY on the given PDF context above
        2. If information is found, mention that it's from the PDF and indicate the serial number of the PDF.
        3. If possible, estimate which page this information might be on (e.g., "beginning," "middle," or "end" of the document, or "page X" if an exact number can be inferred from context, though exact page numbers are not available).
        4. If information is NOT in the context, say clearly "‡§Ø‡§π ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä PDF ‡§Æ‡•á‡§Ç ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä‡•§"
        5. Answer in a natural, helpful way. Ensure all responses are primarily in Nepali if possible.
        
        Answer:
        """
        
        ai_response = call_gemini_smart_improved(enhanced_prompt)
        
        # Step 5: Format the response
        pdf_info = f"PDF #{best_doc['serial_number']} ({best_doc['file_name']})"
        
        # Check if AI found the answer in PDF
        not_found_phrases = ["not found", "‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡§æ", "‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à", "not in the pdf"] # Added Nepali phrase
        if any(phrase in ai_response.lower() for phrase in not_found_phrases):
            # Fallback to general AI answer
            bot.edit_message_text(
                "üì≠ PDF ‡§Æ‡•á‡§Ç ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä ‡§®‡§π‡•Ä‡§Ç ‡§Æ‡§ø‡§≤‡•Ä, AI ‡§∏‡•á ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨ ‡§≤‡•á ‡§∞‡§π‡§æ ‡§π‡•Ç‡§Ç...",
                status_msg.chat.id, 
                status_msg.message_id
            )
            
            general_prompt = f"User asked: {query}\n\nPlease provide a helpful answer based on your general knowledge. Answer in Nepali."
            ai_response = call_gemini_smart_improved(general_prompt)
            
            bot.delete_message(message.chat.id, status_msg.message_id)
            send_long_message(
                message.chat.id,
                f"üìò **AI ‡§ï‡§æ ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§¨:**\n\n"
                f"{ai_response}\n\n"
                f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
        else:
            # Found in PDF - show with PDF info
            bot.delete_message(message.chat.id, status_msg.message_id)
            
            # Try to extract page number from AI response
            # Modified regex to be more flexible and capture page number hints
            page_match = re.search(r'(‡§™‡•á‡§ú\s*\d+|beginning|middle|end)', ai_response.lower())
            page_info = ""
            if page_match:
                page_info = f" ({page_match.group(0)})" # Use the captured group directly
            
            send_long_message(
                message.chat.id,
                f"üìÑ **{pdf_info}{page_info} ‡§ï‡•á ‡§Ü‡§ß‡§æ‡§∞ ‡§™‡§∞:**\n\n"
                f"{ai_response}\n\n"
                f"_‚úÖ ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä PDF ‡§∏‡•á ‡§≤‡•Ä ‡§ó‡§à ‡§π‡•à_",
                reply_to_message_id=message.message_id,
                parse_mode="Markdown"
            )
            
    except Exception as e:
        log_exception(e)
        bot.edit_message_text(
            f"‚ùå ‡§§‡•ç‡§∞‡•Å‡§ü‡§ø: {str(e)[:100]}",
            status_msg.chat.id, 
            status_msg.message_id
        )

def ask_general_ai(message, query, status_msg=None):
    """‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø AI ‡§∏‡•á ‡§ú‡§µ‡§æ‡§¨ ‡§≤‡•á‡§Ç"""
    if status_msg:
                    bot.edit_message_text(
                        "ü§ñ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...",
                        status_msg.chat.id,
                        status_msg.message_id
                    )
    else:
        status_msg = bot.reply_to(message, "ü§ñ AI ‡§¨‡§æ‡§ü ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§® ‡§≤‡§ø‡§Å‡§¶‡•à ‡§õ‡•Å...")
    
    prompt = f"""
    User Question: {query}
    
    Instructions:
    1. Provide a helpful, accurate answer
    2. If you're not sure, say so
    3. Be concise but informative
    4. Answer in Hindi/English as appropriate. Prefer Nepali if context allows.
    
    Answer:
    """
    
    ai_response = call_gemini_smart_improved(prompt)
    
    bot.delete_message(message.chat.id, status_msg.message_id)
    send_long_message(
        message.chat.id,
        f"ü§ñ **AI ‡§ï‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§µ‡§æ‡§´:**\n\n"
        f"{ai_response}\n\n"
        f"_üí° ‡§®‡•ã‡§ü: ‡§Ø‡•ã ‡§ú‡§µ‡§æ‡§´ ‡§Æ‡•á‡§∞‡•ã ‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡§æ‡§®‡§ï‡§æ‡§∞‡•Ä‡§Æ‡§æ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§õ, ‡§ï‡•Å‡§®‡•à ‡§µ‡§ø‡§∂‡•á‡§∑ ‡§´‡§æ‡§á‡§≤‡§¨‡§æ‡§ü ‡§π‡•ã‡§á‡§®‡•§_",
        reply_to_message_id=message.message_id,
        parse_mode="Markdown"
    )

@bot.message_handler(commands=['ask_ai'])
def ask_ai_command(message):
    query = message.text.replace('/ask_ai', '').strip()
    if not query:
        return bot.reply_to(message, "‡§ï‡•É‡§™‡§Ø‡§æ AI ‡§¨‡§æ‡§ü ‡§ï‡•á‡§π‡•Ä ‡§∏‡•ã‡§ß‡•ç‡§®‡•Å‡§π‡•ã‡§∏‡•ç‡•§")
    ask_general_ai(message, query)


@bot.message_handler(commands=['ask_smart'])
def ask_smart(message):
    """‡§∏‡•ç‡§Æ‡§æ‡§∞‡•ç‡§ü‡§≤‡•Ä ‡§°‡§ø‡§∏‡§æ‡§á‡§° ‡§ï‡§∞‡•á‡§Ç - PDF ‡§Æ‡•á‡§Ç ‡§π‡•à ‡§Ø‡§æ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§Ç"""
    query = message.text.replace('/ask_smart', '').strip()
    
    if not query:
        return bot.reply_to(message, "‡§™‡•ç‡§∞‡§∂‡•ç‡§® ‡§¶‡§∞‡•ç‡§ú ‡§ï‡§∞‡•á‡§Ç‡•§")
    
    # First, check if this is a PDF-related question
    pdf_keywords = ['pdf', 'file', 'document', '‡§´‡§æ‡§á‡§≤', '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú', '‡§™‡•Ä‡§°‡•Ä‡§è‡§´']
    
    is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
    
    if is_pdf_question:
        # Use /ask_file logic
        ask_from_file(message) # Note: this calls the modified ask_from_file
    else:
        # Direct AI chat
        handle_chat_improved(message) # This will be the new improved handler

@bot.message_handler(commands=['help'])
def help_command(message):
    help_text = """
    ü§ñ **‡§ï‡§Æ‡§æ‡§Ç‡§°‡•ç‡§∏:**
    
    `/ask_file [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§∏‡§ø‡§∞‡•ç‡§´ PDFs ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú‡•á (‡§Æ‡•à‡§®‡•Å‡§Ö‡§≤ ‡§µ‡•á‡§ï‡•ç‡§ü‡§∞ ‡§ñ‡•ã‡§ú)
    `/ask_ai [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§∏‡§ø‡§∞‡•ç‡§´ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á (‡§∏‡§æ‡§Æ‡§æ‡§®‡•ç‡§Ø ‡§ú‡•ç‡§û‡§æ‡§®)
    `/ask_smart [‡§™‡•ç‡§∞‡§∂‡•ç‡§®]` - ‡§™‡§π‡§≤‡•á PDF ‡§Æ‡•á‡§Ç ‡§ñ‡•ã‡§ú‡•á‡§ó‡§æ ‡§Ö‡§ó‡§∞ ‡§ï‡•Ä‡§µ‡§∞‡•ç‡§° ‡§Æ‡§ø‡§≤‡§§‡•á ‡§π‡•à‡§Ç, ‡§µ‡§∞‡§®‡§æ AI ‡§∏‡•á ‡§™‡•Ç‡§õ‡•á‡§ó‡§æ
    `/quiz [PDF ‡§®‡§Ç‡§¨‡§∞]` - PDF ‡§™‡§∞ ‡§Ü‡§ß‡§æ‡§∞‡§ø‡§§ ‡§ï‡•ç‡§µ‡§ø‡§ú ‡§¨‡§®‡§æ‡§è‡§ó‡§æ
    `/start` - ‡§¨‡•â‡§ü ‡§ï‡§æ ‡§™‡§∞‡§ø‡§ö‡§Ø
    
    **‡§â‡§¶‡§æ‡§π‡§∞‡§£:**
    `/ask_file ‡§á‡§∏ PDF ‡§Æ‡•á‡§Ç ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    `/ask_ai ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§∞‡§æ‡§ú‡§ß‡§æ‡§®‡•Ä ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    `/ask_smart machine learning ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?`
    """
    bot.reply_to(message, help_text, parse_mode="Markdown")

# Modify handle_chat to incorporate the ask_smart logic
@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        # Incorporate ask_smart logic
        query = message.text.strip()
        pdf_keywords = ['pdf', 'file', 'document', '‡§´‡§æ‡§á‡§≤', '‡§¶‡§∏‡•ç‡§§‡§æ‡§µ‡•á‡§ú', '‡§™‡•Ä‡§°‡•Ä‡§è‡§´']
        is_pdf_question = any(keyword in query.lower() for keyword in pdf_keywords)
        
        if is_pdf_question:
            ask_from_file(message)
        else:
            # Original handle_chat logic for general AI
            history = get_chat_history(message.from_user.id)
            bot.send_chat_action(message.chat.id, 'typing')
            res = call_gemini_smart_improved(message.text, history)
            save_chat_history(message.from_user.id, message.text, res)
            send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# ‚Äî‚Äî‚Äî BOT & SERVER RUNNER ‚Äî‚Äî‚Äî

def run_polling():
    """Runs the bot's polling loop in a resilient way."""
    while True:
        try:
            print("ü§ñ Bot Polling Started...")
            bot.infinity_polling(timeout=20, long_polling_timeout=20, skip_pending=True)
        except Exception as e:
            print(f"üí• Polling Crash: {e}")
            log_exception(e)
            print("Restarting polling in 5 seconds...")
            time.sleep(5)

# Start the bot polling in a background thread.
# This runs when the module is imported by Gunicorn.
print("‚úÖ Starting bot polling in a background thread...")
threading.Thread(target=run_polling, daemon=True).start()

# The if __name__ block is now only for local development.
# Gunicorn will not run this, but it will run the code above.
if __name__ == "__main__":
    # When running locally, Flask's dev server is used.
    port = int(os.environ.get("PORT", 10000))
    print(f"Starting Flask dev server on http://0.0.0.0:{port}")
    app.run(host="0.0.0.0", port=port)
import telebot
from telebot.types import Message
from pymongo import MongoClient
import google.generativeai as genai
import fitz  # PyMuPDF
import os
import json
import re
import time
import sys
import traceback

# тАФтАФтАФ рдХрдиреНрдлрд┐рдЧрд░реЗрд╕рди (Render Environment Variables рдмрд╛рдЯ рдкрдвреНрдиреЗ) тАФтАФтАФ
BOT_TOKEN = os.getenv("BOT_TOKEN")
MONGO_URI = os.getenv("MONGO_URI")
API_KEYS = [key.strip() for key in os.getenv("API_KEYS", "").split(',') if key.strip()]
ADMIN_ID = int(os.getenv("ADMIN_ID", 0))
BACKUP_CHANNEL_ID = int(os.getenv("BACKUP_CHANNEL_ID", 0))

DOWNLOAD_PATH = "temp_pdfs"
if not os.path.exists(DOWNLOAD_PATH):
    os.makedirs(DOWNLOAD_PATH)

# тАФтАФтАФ INITIALIZATION (Render-optimized) тАФтАФтАФ
bot = telebot.TeleBot(BOT_TOKEN, threaded=True) # Threaded mode for performance
client = MongoClient(
    MONGO_URI,
    serverSelectionTimeoutMS=5000,
    socketTimeoutMS=20000,
    connectTimeoutMS=20000,
    tlsCAFile=certifi.where()
)
db = client['TelegramBotDB']
pdf_collection = db['PDF_Store']
notes_collection = db['Notes']
counters_collection = db['Counters']
history_collection = db['Chat_History']

MODELS = ["gemini-1.5-flash", "gemini-1.5-pro"]

# тАФтАФтАФ CORE HELPER FUNCTIONS тАФтАФтАФ

def log_exception(e):
    """рд╡рд┐рд╕реНрддреГрдд рддреНрд░реБрдЯрд┐ рд▓рдЧ рдЧрд░реНрдирдХрд╛ рд▓рд╛рдЧрд┐ред"""
    print(f"An exception occurred: {e}")
    traceback.print_exc(file=sys.stdout)

def clean_json(raw_text):
    match = re.search(r'\{.*\}', raw_text, re.DOTALL)
    return match.group(0) if match else raw_text

def get_embedding(text):
    """рдкрд╛рдард▓рд╛рдИ рднреЗрдХреНрдЯрд░ (рдЗрдореНрдмреЗрдбрд┐рдЩ) рдорд╛ рд░реВрдкрд╛рдиреНрддрд░рдг рдЧрд░реНрдЫред"""
    try:
        # Using the more stable embedding-001 model as recommended
        return genai.embed_content(model="models/embedding-001", content=text, task_type="retrieval_document")['embedding']
    except Exception as e:
        print("Embedding error:", e)
        log_exception(e)
        return None

def get_next_serial_number(sequence_name):
    sequence_doc = counters_collection.find_one_and_update(
        {'_id': sequence_name}, {'$inc': {'sequence_value': 1}},
        return_document=True, upsert=True
    )
    return sequence_doc['sequence_value']

def extract_text_from_pdf(file_path):
    try:
        doc = fitz.open(file_path)
        text = "".join(page.get_text() for page in doc)
        doc.close()
        return text if len(text.strip()) >= 100 else None
    except Exception as e:
        print(f"PDF рдкрд╛рда рдирд┐рдХрд╛рд▓реНрджрд╛ рддреНрд░реБрдЯрд┐: {e}")
        log_exception(e)
        return None

def extract_vision_text(file_path):
    img_path = f"temp_scan_{os.path.basename(file_path)}.png"
    try:
        doc = fitz.open(file_path)
        page = doc.load_page(0)
        pix = page.get_pixmap()
        pix.save(img_path)
        doc.close()
        
        model = genai.GenerativeModel('gemini-1.5-flash')
        img_file = genai.upload_file(img_path)
        response = model.generate_content(["Extract all text from this document page:", img_file])
        return response.text
    except Exception as e:
        print(f"Vision OCR рдЕрд╕рдлрд▓ рднрдпреЛ: {e}")
        log_exception(e)
        return None
    finally:
        # FIX: Ensure temp image is always deleted
        if os.path.exists(img_path):
            os.remove(img_path)

def send_long_message(chat_id, text, reply_to_message_id=None, parse_mode="Markdown"):
    if not text: return
    if len(text) <= 4000:
        bot.send_message(chat_id, text, reply_to_message_id=reply_to_message_id, parse_mode=parse_mode)
    else:
        parts = [text[i:i+4000] for i in range(0, len(text), 4000)]
        for part in parts:
            bot.send_message(chat_id, part, parse_mode="Markdown") # reply_to only for first part maybe
            time.sleep(1)

def get_chat_history(user_id):
    history = history_collection.find({"user_id": user_id}).sort("_id", -1).limit(10)
    formatted_history = []
    for msg in reversed(list(history)):
        formatted_history.append({"role": "user", "parts": [msg['user_msg']]})
        formatted_history.append({"role": "model", "parts": [msg['bot_res']]})
    return formatted_history
def save_chat_history(user_id, user_msg, bot_res):
    history_collection.insert_one({"user_id": user_id, "user_msg": user_msg, "bot_res": bot_res})

def call_gemini_smart(prompt, history=[]):
    if not API_KEYS:
        print("API рдХреБрдЮреНрдЬреАрд╣рд░реВ рдХрдиреНрдлрд┐рдЧрд░ рдЧрд░рд┐рдПрдХрд╛ рдЫреИрдирдиреН!")
        return "SERVICE_ERROR: API keys are not configured."
    for key in API_KEYS:
        genai.configure(api_key=key) 
        for model_name in MODELS:
            try:
                model = genai.GenerativeModel(model_name)
                if history:
                    chat = model.start_chat(history=history)
                    response = chat.send_message(prompt)
                else:
                    response = model.generate_content(prompt)
                return response.text
            except Exception as e:
                print(f"рдХреБрдЮреНрдЬреА рдЕрд╕рдлрд▓ рднрдпреЛ: {key[:5]}... рдореЛрдбрд▓: {model_name}")
                log_exception(e)
                time.sleep(1)
                continue
    return "тЭМ All API keys and models failed."


# тАФтАФтАФ BOT MESSAGE HANDLERS тАФтАФтАФ

@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "рдирдорд╕реНрддреЗ! рдо рддрдкрд╛рдИрдВрдХреЛ рдирд┐рдЬреА рдЬреНрдЮрд╛рди рдЖрдзрд╛рд░ (Knowledge Base) рдмреЛрдЯ рд╣реБрдБред")

@bot.message_handler(content_types=['document'])
def handle_pdf_universal(message):
    if message.document.mime_type != 'application/pdf': return
    if message.document.file_size > 20 * 1024 * 1024: return bot.reply_to(message, "рдпреЛ рдлрд╛рдЗрд▓ рдзреЗрд░реИ рдареВрд▓реЛ рдЫ (20MB+)ред")
    if pdf_collection.find_one({"file_id": message.document.file_id}):
        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        return bot.send_message(message.chat.id, f"рдпреЛ PDF рдкрд╣рд┐рд▓реЗ рдиреИ рдмрдЪрдд рдЧрд░рд┐рдПрдХреЛ рдЫред")

    status_msg = bot.send_message(message.chat.id, f"тП│ '{message.document.file_name}' рдкреНрд░рд╢реЛрдзрди рдЧрд░реНрджреИ...")
    file_path = None
    try:
        file_info = bot.get_file(message.document.file_id)
        downloaded_file = bot.download_file(file_info.file_path)
        file_path = os.path.join(DOWNLOAD_PATH, message.document.file_name)
        with open(file_path, 'wb') as new_file: new_file.write(downloaded_file)

        text = extract_text_from_pdf(file_path)
        pdf_type = "digital"
        if not text:
            bot.edit_message_text(f"рдбрд┐рдЬрд┐рдЯрд▓ рдкрд╛рда рдлреЗрд▓рд╛ рдкрд░реЗрди, Vision OCR рдкреНрд░рдпрд╛рд╕ рдЧрд░реНрджреИ...", status_msg.chat.id, status_msg.message_id)
            text = extract_vision_text(file_path)
            pdf_type = "scanned"
        
        if not text: return bot.edit_message_text("тЭМ рдорд╛рдл рдЧрд░реНрдиреБрд╣реЛрд╕реН, рдпреЛ PDF рдмрд╛рдЯ рдХреБрдиреИ рдкрд╛рда рдирд┐рдХрд╛рд▓реНрди рд╕рдХрд┐рдПрдиред", status_msg.chat.id, status_msg.message_id)

        summary_prompt = f"рдпреЛ рд╕рд╛рдордЧреНрд░реАрд▓рд╛рдИ рдЦреЛрдЬ рдЕрдиреБрдХреНрд░рдордгрд┐рдХрд╛рдХреЛ рд▓рд╛рдЧрд┐ реи рд╡рд╛рдХреНрдпрдорд╛ рд╕рд╛рд░рд╛рдВрд╢ рдЧрд░реНрдиреБрд╣реЛрд╕реН: {text[:2000]}"
        summary = call_gemini_smart(summary_prompt)
        vector = get_embedding(summary)
        if not vector: return bot.edit_message_text("тЭМ AI Error: Vector generation failed. Try again.", status_msg.chat.id, status_msg.message_id)
        
        serial_no = get_next_serial_number('pdf_id')
        backup_msg = bot.forward_message(BACKUP_CHANNEL_ID, message.chat.id, message.message_id)
        
        pdf_collection.insert_one({
            "serial_number": serial_no, "file_name": message.document.file_name, "file_id": message.document.file_id,
            "summary": summary, "embedding": vector, "type": pdf_type, "backup_msg_id": backup_msg.message_id,
            "uploader_id": message.from_user.id
        })

        try: bot.delete_message(message.chat.id, message.message_id)
        except: pass
        bot.edit_message_text(f"тЬЕ PDF #{serial_no} ({pdf_type}) '{message.document.file_name}' рд╕рдлрд▓рддрд╛рдкреВрд░реНрд╡рдХ рдкреНрд░рд╢реЛрдзрди рд░ рд╕реБрд░рдХреНрд╖рд┐рдд рдЧрд░рд┐рдпреЛред", status_msg.chat.id, status_msg.message_id)

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"рдорд╛рдл рдЧрд░реНрдиреБрд╣реЛрд╕реН, рдлрд╛рдЗрд▓ рдкреНрд░рд╢реЛрдзрди рдЧрд░реНрджрд╛ рддреНрд░реБрдЯрд┐ рдЖрдпреЛ: {e}", status_msg.chat.id, status_msg.message_id)
    finally:
        if file_path and os.path.exists(file_path): os.remove(file_path)

@bot.message_handler(commands=['get'])
def retrieve_pdf(message):
    if message.from_user.id != ADMIN_ID:
        return bot.reply_to(message, "тЭМ рд╕реБрд░рдХреНрд╖рд╛ рдХрд╛рд░рдгрд▓реЗ рдЧрд░реНрджрд╛ PDF рдлрд╛рдЗрд▓ рдбрд╛рдЙрдирд▓реЛрдб рдЧрд░реНрди рдЕрдиреБрдорддрд┐ рдЫреИрдиред рддрдкрд╛рдИрдВ рдпрд╕рдХреЛ рдмрд╛рд░реЗрдорд╛ AI рд╕рдБрдЧ рд╕реЛрдзреНрди рд╕рдХреНрдиреБрд╣реБрдиреНрдЫред")
    if message.chat.type != 'private':
        try: bot.send_message(message.from_user.id, "ЁЯЫбя╕П рд╕реБрд░рдХреНрд╖рд╛рдХрд╛ рд▓рд╛рдЧрд┐, рдо рддрдкрд╛рдИрдВрд▓рд╛рдИ рдпреЛ рдлрд╛рдЗрд▓ Private Message (PM) рдорд╛ рдкрдард╛рдЙрдБрджреИрдЫреБред")
        except: return bot.reply_to(message, "Please start a chat with me privately first so I can PM you.")
        return bot.reply_to(message, "ЁЯЫбя╕П рдо рддрдкрд╛рдИрдВрд▓рд╛рдИ рдпреЛ рдлрд╛рдЗрд▓ PM рдорд╛ рдкрдард╛рдЙрдБрджреИрдЫреБред")

    try:
        args = message.text.split()
        if len(args) < 2: return bot.reply_to(message, "рдирдореНрдмрд░ рджрд┐рдиреБрд╣реЛрд╕реНред Ex: /get 1")
        index_no = int(args[1])
        res = pdf_collection.find_one({"serial_number": index_no})
        if res: bot.send_document(ADMIN_ID, res['file_id'], caption=f"ЁЯУД Admin Copy: {res['file_name']}")
        else: bot.reply_to(message, "рдлрд╛рдЗрд▓ рднреЗрдЯрд┐рдПрдиред")
    except Exception as e: 
        log_exception(e)
        bot.reply_to(message, f"рддреНрд░реБрдЯрд┐ рднрдпреЛ: {e}")

@bot.message_handler(commands=['ask_file'])
def ask_from_file(message):
    query = message.text.replace('/ask_file', '').strip()
    if not query: return bot.reply_to(message, "рдХреГрдкрдпрд╛ рдлрд╛рдЗрд▓рдХреЛ рдмрд╛рд░реЗрдорд╛ рдХреЗрд╣реА рд╕реЛрдзреНрдиреБрд╣реЛрд╕реНред")
    status_msg = bot.reply_to(message, "ЁЯФН рдлрд╛рдЗрд▓рд╣рд░реВрдорд╛ рдЦреЛрдЬреНрджреИ...")
    try:
        vector = get_embedding(query)
        if not vector:
            return bot.edit_message_text("тЭМ AI Error: рддрдкрд╛рдИрдВрдХреЛ рдкреНрд░рд╢реНрдирдХреЛ рд▓рд╛рдЧрд┐ рднреЗрдХреНрдЯрд░ рдмрдирд╛рдЙрди рд╕рдХрд┐рдПрдиред рдХреГрдкрдпрд╛ рдЖрдлреНрдиреЛ API рдХреБрдЮреНрдЬреАрд╣рд░реВ рдЬрд╛рдБрдЪ рдЧрд░реНрдиреБрд╣реЛрд╕реНред", status_msg.chat.id, status_msg.message_id)
        
        pipeline = [{"$vectorSearch": {"index": "vector_index", "path": "embedding", "queryVector": vector, "numCandidates": 10, "limit": 1}}]
        results = list(pdf_collection.aggregate(pipeline))
        if not results: return bot.edit_message_text("тЭМ рд╕рдореНрдмрдиреНрдзрд┐рдд рдЬрд╛рдирдХрд╛рд░реА рднреЗрдЯрд┐рдПрдиред", message.chat.id, status_msg.message_id)
        
        context = results[0]['summary']
        prompt = f"Context from PDF: {context}\n\nUser Question: {query}\n\nAnswer based on context only:"
        
        bot.edit_message_text("тЬНя╕П рд╕рд╛рдиреНрджрд░реНрднрд┐рдХ рдЬрд╛рдирдХрд╛рд░реА рднреЗрдЯрд┐рдпреЛ, рдЬрд╡рд╛рдл рддрдпрд╛рд░ рдкрд╛рд░реНрджреИ...", status_msg.chat.id, status_msg.message_id)
        ai_response = call_gemini_smart(prompt)

        bot.delete_message(message.chat.id, status_msg.message_id)
        send_long_message(message.chat.id, f"ЁЯУД **рдлрд╛рдЗрд▓рдХреЛ рдЖрдзрд╛рд░рдорд╛ рдЬрд╡рд╛рдл:**\n\n{ai_response}", reply_to_message_id=message.message_id, parse_mode="Markdown")

    except Exception as e:
        log_exception(e)
        bot.edit_message_text(f"рддреНрд░реБрдЯрд┐: {e}", status_msg.chat.id, status_msg.message_id)

@bot.message_handler(commands=['quiz'])
def generate_pdf_quiz(message):
    args = message.text.split()
    if len(args) < 2: return bot.reply_to(message, "рдХреГрдкрдпрд╛ PDF рдирдореНрдмрд░ рджрд┐рдиреБрд╣реЛрд╕реНред рдЙрджрд╛рд╣рд░рдг: `/quiz 1`")
    try:
        pdf_id = int(args[1])
        res = pdf_collection.find_one({"serial_number": pdf_id})
        if not res: return bot.reply_to(message, "рдпреЛ рдирдореНрдмрд░рдХреЛ рдлрд╛рдЗрд▓ рднреЗрдЯрд┐рдПрдиред")

        status_msg = bot.reply_to(message, f"тП│ {res['file_name']} рдмрд╛рдЯ рдХреНрд╡рд┐рдЬ рддрдпрд╛рд░ рдЧрд░реНрджреИрдЫреБ...")
        prompt = f"Create 1 MCQ quiz in JSON based on this: {res['summary']}. Return only JSON."
        ai_res = call_gemini_smart(prompt)
        data = json.loads(clean_json(ai_res))
        bot.send_poll(message.chat.id, question=data['question'][:255], options=[o[:100] for o in data['options']], correct_option_id=data['correct_option_id'], type='quiz', explanation=data.get('explanation', '')[:200])
        bot.delete_message(message.chat.id, status_msg.message_id)
    except Exception as e: 
        log_exception(e)
        bot.edit_message_text(f"рддреНрд░реБрдЯрд┐: {e}", message.chat.id, status_msg.message_id if 'status_msg' in locals() else message.message_id)

@bot.message_handler(func=lambda message: not message.text.startswith('/'))
def handle_chat(message):
    if message.chat.type == 'private' or (message.reply_to_message and message.reply_to_message.from_user.id == bot.get_me().id):
        history = get_chat_history(message.from_user.id)
        bot.send_chat_action(message.chat.id, 'typing')
        res = call_gemini_smart(message.text, history)
        save_chat_history(message.from_user.id, message.text, res)
        # FIX: Use the correct variable 'res' instead of 'bot_response'
        send_long_message(message.chat.id, res, reply_to_message_id=message.message_id)

# тАФтАФтАФ BOT START (Render Safe) тАФтАФтАФ
if __name__ == "__main__":
    print("Bot started...")
    # These timeouts prevent the bot from getting stuck
    bot.infinity_polling(
        skip_pending=True,
        timeout=30,
        long_polling_timeout=30
    )